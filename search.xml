<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Spring Retry重试实现原理]]></title>
    <url>%2Fposts%2F69a9647f%2F</url>
    <content type="text"><![CDATA[概要Spring实现了一套重试机制，功能简单实用。Spring Retry是从Spring Batch独立出来的一个功能，已经广泛应用于Spring Batch,Spring Integration, Spring for Apache Hadoop等Spring项目。本文将讲述如何使用Spring Retry及其实现原理。背景重试，其实我们其实很多时候都需要的，为了保证容错性，可用性，一致性等。一般用来应对外部系统的一些不可预料的返回、异常等，特别是网络延迟，中断等情况。还有在现在流行的微服务治理框架中，通常都有自己的重试与超时配置，比如dubbo可以设置retries=1，timeout=500调用失败只重试1次，超过500ms调用仍未返回则调用失败。如果我们要做重试，要为特定的某个操作做重试功能，则要硬编码，大概逻辑基本都是写个循环，根据返回或异常，计数失败次数，然后设定退出条件。 这样做，且不说每个操作都要写这种类似的代码，而且重试逻辑和业务逻辑混在一起，给维护和扩展带来了麻烦。从面向对象的角度来看，我们应该把重试的代码独立出来。使用介绍基本使用先举个例子：1234567891011121314151617181920212223242526272829303132@Configuration@EnableRetrypublic class Application &#123; @Bean public RetryService retryService()&#123; return new RetryService(); &#125; public static void main(String[] args) throws Exception&#123; ApplicationContext applicationContext = new AnnotationConfigApplicationContext("springretry"); RetryService service1 = applicationContext.getBean("service", RetryService.class); service1.service(); &#125;&#125;@Service("service")public class RetryService &#123; @Retryable(value = IllegalAccessException.class, maxAttempts = 5, backoff= @Backoff(value = 1500, maxDelay = 100000, multiplier = 1.2)) public void service() throws IllegalAccessException &#123; System.out.println("service method..."); throw new IllegalAccessException("manual exception"); &#125; @Recover public void recover(IllegalAccessException e)&#123; System.out.println("service retry after Recover =&gt; " + e.getMessage()); &#125;&#125;@EnableRetry - 表示开启重试机制@Retryable - 表示这个方法需要重试，它有很丰富的参数，可以满足你对重试的需求@Backoff - 表示重试中的退避策略@Recover - 兜底方法，即多次重试后还是失败就会执行这个方法Spring-Retry 的功能丰富在于其重试策略和退避策略，还有兜底，监听器等操作。然后每个注解里面的参数，都是很简单的，大家看一下就知道是什么意思，怎么用了，我就不多讲了。重试策略看一下Spring Retry自带的一些重试策略，主要是用来判断当方法调用异常时是否需要重试。（下文原理部分会深入分析实现）SimpleRetryPolicy默认最多重试3次TimeoutRetryPolicy默认在1秒内失败都会重试ExpressionRetryPolicy符合表达式就会重试CircuitBreakerRetryPolicy增加了熔断的机制，如果不在熔断状态，则允许重试CompositeRetryPolicy可以组合多个重试策略NeverRetryPolicy从不重试（也是一种重试策略哈）AlwaysRetryPolicy总是重试….等等退避策略看一下退避策略，退避是指怎么去做下一次的重试，在这里其实就是等待多长时间。（下文原理部分会深入分析实现）FixedBackOffPolicy默认固定延迟1秒后执行下一次重试ExponentialBackOffPolicy指数递增延迟执行重试，默认初始0.1秒，系数是2，那么下次延迟0.2秒，再下次就是延迟0.4秒，如此类推，最大30秒。ExponentialRandomBackOffPolicy在上面那个策略上增加随机性UniformRandomBackOffPolicy这个跟上面的区别就是，上面的延迟会不停递增，这个只会在固定的区间随机StatelessBackOffPolicy这个说明是无状态的，所谓无状态就是对上次的退避无感知，从它下面的子类也能看出来原理原理部分我想分开两部分来讲，一是重试机制的切入点，即它是如何使得你的代码实现重试功能的；二是重试机制的详细，包括重试的逻辑以及重试策略和退避策略的实现。切入点@EnableRetry12345678910111213141516@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@EnableAspectJAutoProxy(proxyTargetClass = false)@Import(RetryConfiguration.class)@Documentedpublic @interface EnableRetry &#123; /** * Indicate whether subclass-based (CGLIB) proxies are to be created as opposed * to standard Java interface-based proxies. The default is &#123;@code false&#125;. * * @return whether to proxy or not to proxy the class */ boolean proxyTargetClass() default false;&#125;我们可以看到@EnableAspectJAutoProxy(proxyTargetClass = false)这个并不陌生，就是打开Spring AOP功能。重点看看@Import(RetryConfiguration.class)@Import相当于注册这个Bean我们看看这个RetryConfiguration是个什么东西它是一个AbstractPointcutAdvisor，它有一个pointcut和一个advice。我们知道，在IOC过程中会根据PointcutAdvisor类来对Bean进行Pointcut的过滤，然后生成对应的AOP代理类，用advice来加强处理。看看RetryConfiguration的初始化:123456789101112@PostConstruct public void init() &#123; Set&lt;Class&lt;? extends Annotation&gt;&gt; retryableAnnotationTypes = new LinkedHashSet&lt;Class&lt;? extends Annotation&gt;&gt;(1); retryableAnnotationTypes.add(Retryable.class); //创建pointcut this.pointcut = buildPointcut(retryableAnnotationTypes); //创建advice this.advice = buildAdvice(); if (this.advice instanceof BeanFactoryAware) &#123; ((BeanFactoryAware) this.advice).setBeanFactory(beanFactory); &#125; &#125;12345678910111213protected Pointcut buildPointcut(Set&lt;Class&lt;? extends Annotation&gt;&gt; retryAnnotationTypes) &#123; ComposablePointcut result = null; for (Class&lt;? extends Annotation&gt; retryAnnotationType : retryAnnotationTypes) &#123; Pointcut filter = new AnnotationClassOrMethodPointcut(retryAnnotationType); if (result == null) &#123; result = new ComposablePointcut(filter); &#125; else &#123; result.union(filter); &#125; &#125; return result; &#125;上面代码用到了AnnotationClassOrMethodPointcut，其实它最终还是用到了AnnotationMethodMatcher来根据注解进行切入点的过滤。这里就是@Retryable注解了。123456789101112131415161718192021//创建advice对象，即拦截器 protected Advice buildAdvice() &#123; //下面关注这个对象 AnnotationAwareRetryOperationsInterceptor interceptor = new AnnotationAwareRetryOperationsInterceptor(); if (retryContextCache != null) &#123; interceptor.setRetryContextCache(retryContextCache); &#125; if (retryListeners != null) &#123; interceptor.setListeners(retryListeners); &#125; if (methodArgumentsKeyGenerator != null) &#123; interceptor.setKeyGenerator(methodArgumentsKeyGenerator); &#125; if (newMethodArgumentsIdentifier != null) &#123; interceptor.setNewItemIdentifier(newMethodArgumentsIdentifier); &#125; if (sleeper != null) &#123; interceptor.setSleeper(sleeper); &#125; return interceptor;&#125;AnnotationAwareRetryOperationsInterceptor继承关系可以看出AnnotationAwareRetryOperationsInterceptor是一个MethodInterceptor，在创建AOP代理过程中如果目标方法符合pointcut的规则，它就会加到interceptor列表中，然后做增强，我们看看invoke方法做了什么增强。12345678910@Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; MethodInterceptor delegate = getDelegate(invocation.getThis(), invocation.getMethod()); if (delegate != null) &#123; return delegate.invoke(invocation); &#125; else &#123; return invocation.proceed(); &#125; &#125;这里用到了委托，主要是需要根据配置委托给具体“有状态”的interceptor还是“无状态”的interceptor。12345678910111213141516171819202122232425262728293031323334353637private MethodInterceptor getDelegate(Object target, Method method) &#123; if (!this.delegates.containsKey(target) || !this.delegates.get(target).containsKey(method)) &#123; synchronized (this.delegates) &#123; if (!this.delegates.containsKey(target)) &#123; this.delegates.put(target, new HashMap&lt;Method, MethodInterceptor&gt;()); &#125; Map&lt;Method, MethodInterceptor&gt; delegatesForTarget = this.delegates.get(target); if (!delegatesForTarget.containsKey(method)) &#123; Retryable retryable = AnnotationUtils.findAnnotation(method, Retryable.class); if (retryable == null) &#123; retryable = AnnotationUtils.findAnnotation(method.getDeclaringClass(), Retryable.class); &#125; if (retryable == null) &#123; retryable = findAnnotationOnTarget(target, method); &#125; if (retryable == null) &#123; return delegatesForTarget.put(method, null); &#125; MethodInterceptor delegate; //支持自定义MethodInterceptor，而且优先级最高 if (StringUtils.hasText(retryable.interceptor())) &#123; delegate = this.beanFactory.getBean(retryable.interceptor(), MethodInterceptor.class); &#125; else if (retryable.stateful()) &#123; //得到“有状态”的interceptor delegate = getStatefulInterceptor(target, method, retryable); &#125; else &#123; //得到“无状态”的interceptor delegate = getStatelessInterceptor(target, method, retryable); &#125; delegatesForTarget.put(method, delegate); &#125; &#125; &#125; return this.delegates.get(target).get(method); &#125;getStatefulInterceptor和getStatelessInterceptor都是差不多，我们先看看比较简单的getStatelessInterceptor。12345678910111213private MethodInterceptor getStatelessInterceptor(Object target, Method method, Retryable retryable) &#123; //生成一个RetryTemplate RetryTemplate template = createTemplate(retryable.listeners()); //生成retryPolicy template.setRetryPolicy(getRetryPolicy(retryable)); //生成backoffPolicy template.setBackOffPolicy(getBackoffPolicy(retryable.backoff())); return RetryInterceptorBuilder.stateless() .retryOperations(template) .label(retryable.label()) .recoverer(getRecoverer(target, method)) .build(); &#125;具体生成retryPolicy和backoffPolicy的规则，我们等下再回头来看。RetryInterceptorBuilder其实就是为了生成RetryOperationsInterceptor。RetryOperationsInterceptor也是一个MethodInterceptor，我们来看看它的invoke方法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public Object invoke(final MethodInvocation invocation) throws Throwable &#123; String name; if (StringUtils.hasText(label)) &#123; name = label; &#125; else &#123; name = invocation.getMethod().toGenericString(); &#125; final String label = name; //定义了一个RetryCallback，其实看它的doWithRetry方法，调用了invocation的proceed()方法，是不是有点眼熟，这就是AOP的拦截链调用，如果没有拦截链，那就是对原来方法的调用。 RetryCallback&lt;Object, Throwable&gt; retryCallback = new RetryCallback&lt;Object, Throwable&gt;() &#123; public Object doWithRetry(RetryContext context) throws Exception &#123; context.setAttribute(RetryContext.NAME, label); /* * If we don't copy the invocation carefully it won't keep a reference to * the other interceptors in the chain. We don't have a choice here but to * specialise to ReflectiveMethodInvocation (but how often would another * implementation come along?). */ if (invocation instanceof ProxyMethodInvocation) &#123; try &#123; return ((ProxyMethodInvocation) invocation).invocableClone().proceed(); &#125; catch (Exception e) &#123; throw e; &#125; catch (Error e) &#123; throw e; &#125; catch (Throwable e) &#123; throw new IllegalStateException(e); &#125; &#125; else &#123; throw new IllegalStateException( "MethodInvocation of the wrong type detected - this should not happen with Spring AOP, " + "so please raise an issue if you see this exception"); &#125; &#125; &#125;; if (recoverer != null) &#123; ItemRecovererCallback recoveryCallback = new ItemRecovererCallback( invocation.getArguments(), recoverer); return this.retryOperations.execute(retryCallback, recoveryCallback); &#125; //最终还是进入到retryOperations的execute方法，这个retryOperations就是在之前的builder set进来的RetryTemplate。 return this.retryOperations.execute(retryCallback); &#125;无论是RetryOperationsInterceptor还是StatefulRetryOperationsInterceptor，最终的拦截处理逻辑还是调用到RetryTemplate的execute方法，从名字也看出来，RetryTemplate作为一个模板类，里面包含了重试统一逻辑。不过，我看这个RetryTemplate并不是很“模板”，因为它没有很多可以扩展的地方。重试逻辑及策略实现上面介绍了Spring Retry利用了AOP代理使重试机制对业务代码进行“入侵”。下面我们继续看看重试的逻辑做了什么。RetryTemplate的doExecute方法。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139protected &lt;T, E extends Throwable&gt; T doExecute(RetryCallback&lt;T, E&gt; retryCallback, RecoveryCallback&lt;T&gt; recoveryCallback, RetryState state) throws E, ExhaustedRetryException &#123; RetryPolicy retryPolicy = this.retryPolicy; BackOffPolicy backOffPolicy = this.backOffPolicy; //新建一个RetryContext来保存本轮重试的上下文 RetryContext context = open(retryPolicy, state); if (this.logger.isTraceEnabled()) &#123; this.logger.trace("RetryContext retrieved: " + context); &#125; // Make sure the context is available globally for clients who need // it... RetrySynchronizationManager.register(context); Throwable lastException = null; boolean exhausted = false; try &#123; //如果有注册RetryListener，则会调用它的open方法，给调用者一个通知。 boolean running = doOpenInterceptors(retryCallback, context); if (!running) &#123; throw new TerminatedRetryException( "Retry terminated abnormally by interceptor before first attempt"); &#125; // Get or Start the backoff context... BackOffContext backOffContext = null; Object resource = context.getAttribute("backOffContext"); if (resource instanceof BackOffContext) &#123; backOffContext = (BackOffContext) resource; &#125; if (backOffContext == null) &#123; backOffContext = backOffPolicy.start(context); if (backOffContext != null) &#123; context.setAttribute("backOffContext", backOffContext); &#125; &#125; //判断能否重试，就是调用RetryPolicy的canRetry方法来判断。 //这个循环会直到原方法不抛出异常，或不需要再重试 while (canRetry(retryPolicy, context) &amp;&amp; !context.isExhaustedOnly()) &#123; try &#123; if (this.logger.isDebugEnabled()) &#123; this.logger.debug("Retry: count=" + context.getRetryCount()); &#125; //清除上次记录的异常 lastException = null; //doWithRetry方法，一般来说就是原方法 return retryCallback.doWithRetry(context); &#125; catch (Throwable e) &#123; //原方法抛出了异常 lastException = e; try &#123; //记录异常信息 registerThrowable(retryPolicy, state, context, e); &#125; catch (Exception ex) &#123; throw new TerminatedRetryException("Could not register throwable", ex); &#125; finally &#123; //调用RetryListener的onError方法 doOnErrorInterceptors(retryCallback, context, e); &#125; //再次判断能否重试 if (canRetry(retryPolicy, context) &amp;&amp; !context.isExhaustedOnly()) &#123; try &#123; //如果可以重试则走退避策略 backOffPolicy.backOff(backOffContext); &#125; catch (BackOffInterruptedException ex) &#123; lastException = e; // back off was prevented by another thread - fail the retry if (this.logger.isDebugEnabled()) &#123; this.logger .debug("Abort retry because interrupted: count=" + context.getRetryCount()); &#125; throw ex; &#125; &#125; if (this.logger.isDebugEnabled()) &#123; this.logger.debug( "Checking for rethrow: count=" + context.getRetryCount()); &#125; if (shouldRethrow(retryPolicy, context, state)) &#123; if (this.logger.isDebugEnabled()) &#123; this.logger.debug("Rethrow in retry for policy: count=" + context.getRetryCount()); &#125; throw RetryTemplate.&lt;E&gt;wrapIfNecessary(e); &#125; &#125; /* * A stateful attempt that can retry may rethrow the exception before now, * but if we get this far in a stateful retry there's a reason for it, * like a circuit breaker or a rollback classifier. */ if (state != null &amp;&amp; context.hasAttribute(GLOBAL_STATE)) &#123; break; &#125; &#125; if (state == null &amp;&amp; this.logger.isDebugEnabled()) &#123; this.logger.debug( "Retry failed last attempt: count=" + context.getRetryCount()); &#125; exhausted = true; //重试结束后如果有兜底Recovery方法则执行，否则抛异常 return handleRetryExhausted(recoveryCallback, context, state); &#125; catch (Throwable e) &#123; throw RetryTemplate.&lt;E&gt;wrapIfNecessary(e); &#125; finally &#123; //处理一些关闭逻辑 close(retryPolicy, context, state, lastException == null || exhausted); //调用RetryListener的close方法 doCloseInterceptors(retryCallback, context, lastException); RetrySynchronizationManager.clear(); &#125; &#125;主要核心重试逻辑就是上面的代码了，看上去还是挺简单的。在上面，我们漏掉了RetryPolicy的canRetry方法和BackOffPolicy的backOff方法，以及这两个Policy是怎么来的。我们回头看看getStatelessInterceptor方法中的getRetryPolicy和getRetryPolicy方法。123456789101112131415161718192021222324252627282930313233343536373839404142private RetryPolicy getRetryPolicy(Annotation retryable) &#123; Map&lt;String, Object&gt; attrs = AnnotationUtils.getAnnotationAttributes(retryable); @SuppressWarnings("unchecked") Class&lt;? extends Throwable&gt;[] includes = (Class&lt;? extends Throwable&gt;[]) attrs.get("value"); String exceptionExpression = (String) attrs.get("exceptionExpression"); boolean hasExpression = StringUtils.hasText(exceptionExpression); if (includes.length == 0) &#123; @SuppressWarnings("unchecked") Class&lt;? extends Throwable&gt;[] value = (Class&lt;? extends Throwable&gt;[]) attrs.get("include"); includes = value; &#125; @SuppressWarnings("unchecked") Class&lt;? extends Throwable&gt;[] excludes = (Class&lt;? extends Throwable&gt;[]) attrs.get("exclude"); Integer maxAttempts = (Integer) attrs.get("maxAttempts"); String maxAttemptsExpression = (String) attrs.get("maxAttemptsExpression"); if (StringUtils.hasText(maxAttemptsExpression)) &#123; maxAttempts = PARSER.parseExpression(resolve(maxAttemptsExpression), PARSER_CONTEXT) .getValue(this.evaluationContext, Integer.class); &#125; if (includes.length == 0 &amp;&amp; excludes.length == 0) &#123; SimpleRetryPolicy simple = hasExpression ? new ExpressionRetryPolicy(resolve(exceptionExpression)) .withBeanFactory(this.beanFactory) : new SimpleRetryPolicy(); simple.setMaxAttempts(maxAttempts); return simple; &#125; Map&lt;Class&lt;? extends Throwable&gt;, Boolean&gt; policyMap = new HashMap&lt;Class&lt;? extends Throwable&gt;, Boolean&gt;(); for (Class&lt;? extends Throwable&gt; type : includes) &#123; policyMap.put(type, true); &#125; for (Class&lt;? extends Throwable&gt; type : excludes) &#123; policyMap.put(type, false); &#125; boolean retryNotExcluded = includes.length == 0; if (hasExpression) &#123; return new ExpressionRetryPolicy(maxAttempts, policyMap, true, exceptionExpression, retryNotExcluded) .withBeanFactory(this.beanFactory); &#125; else &#123; return new SimpleRetryPolicy(maxAttempts, policyMap, true, retryNotExcluded); &#125; &#125;嗯～，代码不难，这里简单做一下总结好了。就是通过@Retryable注解中的参数，来判断具体使用文章开头说到的哪个重试策略，是SimpleRetryPolicy还是ExpressionRetryPolicy等。123456789101112131415161718192021222324252627282930313233343536373839404142434445private BackOffPolicy getBackoffPolicy(Backoff backoff) &#123; long min = backoff.delay() == 0 ? backoff.value() : backoff.delay(); if (StringUtils.hasText(backoff.delayExpression())) &#123; min = PARSER.parseExpression(resolve(backoff.delayExpression()), PARSER_CONTEXT) .getValue(this.evaluationContext, Long.class); &#125; long max = backoff.maxDelay(); if (StringUtils.hasText(backoff.maxDelayExpression())) &#123; max = PARSER.parseExpression(resolve(backoff.maxDelayExpression()), PARSER_CONTEXT) .getValue(this.evaluationContext, Long.class); &#125; double multiplier = backoff.multiplier(); if (StringUtils.hasText(backoff.multiplierExpression())) &#123; multiplier = PARSER.parseExpression(resolve(backoff.multiplierExpression()), PARSER_CONTEXT) .getValue(this.evaluationContext, Double.class); &#125; if (multiplier &gt; 0) &#123; ExponentialBackOffPolicy policy = new ExponentialBackOffPolicy(); if (backoff.random()) &#123; policy = new ExponentialRandomBackOffPolicy(); &#125; policy.setInitialInterval(min); policy.setMultiplier(multiplier); policy.setMaxInterval(max &gt; min ? max : ExponentialBackOffPolicy.DEFAULT_MAX_INTERVAL); if (this.sleeper != null) &#123; policy.setSleeper(this.sleeper); &#125; return policy; &#125; if (max &gt; min) &#123; UniformRandomBackOffPolicy policy = new UniformRandomBackOffPolicy(); policy.setMinBackOffPeriod(min); policy.setMaxBackOffPeriod(max); if (this.sleeper != null) &#123; policy.setSleeper(this.sleeper); &#125; return policy; &#125; FixedBackOffPolicy policy = new FixedBackOffPolicy(); policy.setBackOffPeriod(min); if (this.sleeper != null) &#123; policy.setSleeper(this.sleeper); &#125; return policy; &#125;嗯～，一样的味道。就是通过@Backoff注解中的参数，来判断具体使用文章开头说到的哪个退避策略，是FixedBackOffPolicy还是UniformRandomBackOffPolicy等。那么每个RetryPolicy都会重写canRetry方法，然后在RetryTemplate判断是否需要重试。我们看看SimpleRetryPolicy的1234567@Override public boolean canRetry(RetryContext context) &#123; Throwable t = context.getLastThrowable(); //判断抛出的异常是否符合重试的异常 //还有，是否超过了重试的次数 return (t == null || retryForException(t)) &amp;&amp; context.getRetryCount() &lt; maxAttempts; &#125;同样，我们看看FixedBackOffPolicy的退避方法。123456789protected void doBackOff() throws BackOffInterruptedException &#123; try &#123; //就是sleep固定的时间 sleeper.sleep(backOffPeriod); &#125; catch (InterruptedException e) &#123; throw new BackOffInterruptedException("Thread interrupted while sleeping", e); &#125; &#125;至此，重试的主要原理以及逻辑大概就是这样了。RetryContext我觉得有必要说说RetryContext，先看看它的继承关系。可以看出对每一个策略都有对应的Context。在Spring Retry里，其实每一个策略都是单例来的。我刚开始直觉是对每一个需要重试的方法都会new一个策略，这样重试策略之间才不会产生冲突，但是一想就知道这样就可能多出了很多策略对象出来，增加了使用者的负担，这不是一个好的设计。Spring Retry采用了一个更加轻量级的做法，就是针对每一个需要重试的方法只new一个上下文Context对象，然后在重试时，把这个Context传到策略里，策略再根据这个Context做重试，而且Spring Retry还对这个Context做了cache。这样就相当于对重试的上下文做了优化。总结Spring Retry通过AOP机制来实现对业务代码的重试”入侵“，RetryTemplate中包含了核心的重试逻辑，还提供了丰富的重试策略和退避策略。参考资料http://www.10tiao.com/html/164/201705/2652898434/1.htmlhttps://www.jianshu.com/p/58e753ca0151https://paper.tuisec.win/detail/90bd660fad92183]]></content>
      <categories>
        <category>java</category>
        <category>spring</category>
      </categories>
      <tags>
        <tag>spring-retry</tag>
        <tag>retry</tag>
        <tag>重试机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[让线程“暂停”的方式对比]]></title>
    <url>%2Fposts%2F4a4dcc9%2F</url>
    <content type="text"><![CDATA[概要其实对这四种方式都知道，但是有时看到人家代码时就会想为什么要采用这种方式来阻塞呢？用其他的行不行？问题其实是对这四种方式的使用场景不熟悉，我从编程的角度来整理一下。介绍yieldThread.yield()大家都知道，它会放弃当前的CPU时间片，退下来让给别的线程运行，但你看它是没有时间参数的，所以这个线程下次被调度的时间点是不定的。如果对暂停的时间没有要求，可以使用这个，你并不要期望这个线程能暂停多久。sleepThread.sleep()大家都知道，效果跟Thread.yield()一样，不过sleep可以指定时间，什么时候醒过来继续运行。JDK1.5开始推荐使用TimeUnit的sleep方法，其实是一样的，只是增强了可读性。wait / notifywait 和 notify 是基于对象锁的竞争。大家都知道，synchronized关键让进入临界区的线程获得了该对象的锁，即在Mark Word头写上这个线程的ID（偏向锁），如果其他线程来了，会先检查有没有线程在使用这个锁。如果要使用wait / notify，你得先有一个临界区，进入临界区的线程有资格，其他的线程需要“暂停”。所以其实你的操作都是围绕这个对象锁的，你对线程是没有感知的。park / unparkLockSupport的park和unpark是基于对线程暂停和唤醒，这听上去有点废话，大家其实可以理解为是对线程的“命令”，确实大多数网上对unpark的翻译是“许可”，即叫你停就停，动就动。既然你要操作线程，你必然要能够感知到所有相关的线程的。总结漏了ReentrantLock/Condition，其实这跟wait / notify差不多，也是基于“对象锁”的；yield 自己随意“暂停”；sleep 自己有目的的“暂停”；wait / notify 基于对象锁做运行或等待；park / unpark 基于（其他）线程的“命令”，所以它没有一个“锁”的概念；]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>阻塞</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解Future及FutureTask的实现]]></title>
    <url>%2Fposts%2F5dd69f23%2F</url>
    <content type="text"><![CDATA[概要Future是一种异步计算的模式，本文带你理解一下什么是Future，以及基本的FutureTask的实现原理。作用如果在一个方法中要执行另一个操作（任务），但是这个操作会耗时很久，而且你后面还需要用到这个操作的返回结果或者必须等到这个操作结束你才能走下去，你会怎样做？可能大家都会想到异步去执行，即新建一个线程去做这个事情，但是这样的话，你后面的操作就要放到这个异步线程那里，你的方法就变成异步的了，对你原来的返回造成了影响。这时候，Future就发挥作用了，有些地方说它是一种模式，其实，它就是对一个异步操作的封装，它会返回一个“凭证”给你，你可以用这个“凭证”在需要的时候获取到这个异步操作的结果，一般来说这个“凭证”就是future。原理FutureTask就是Future的基本实现，下面我们就从代码分析一下实现的原理。源码基本JDK1.8。Future接口我们先看看Future的定义，即你拿到这个“凭证”之后你能干点什么。1234567891011121314public interface Future&lt;V&gt; &#123; //取消这次任务 boolean cancel(boolean mayInterruptIfRunning); //看看是否取消了 boolean isCancelled(); //看看是否完成了 boolean isDone(); //获取结果 V get() throws InterruptedException, ExecutionException; //时间内获取结果，超时则抛异常 V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException;&#125;使用如果没用过的，这里简单演示一下Future怎么使用，大家有个感性认识。123456789101112131415161718private static ExecutorService threadPool = Executors.newCachedThreadPool(); public static void main(String[] args) throws Exception &#123; //通过线程池提交任务，并返回一个future Future&lt;String&gt; future = threadPool.submit(new AsyncTask()); //通过future获取结果。get之前一直阻塞直到有结果返回。 String result = future.get(); System.out.println(result); &#125; public class AsyncTask implements Callable&lt;String&gt; &#123; @Override public String call() throws Exception &#123; TimeUnit.SECONDS.sleep(5); return "ok"; &#125; &#125;继承关系FutureTask是一个RunnableFuture，这个很好理解，就是Runnable+Future了。提交任务从任务的提交入手分析源码。AbstractExecutorService的submit方法。123456789public &lt;T&gt; Future&lt;T&gt; submit(Callable&lt;T&gt; task) &#123; if (task == null) throw new NullPointerException(); //这里封装了一下callable变成一个RunnableFuture RunnableFuture&lt;T&gt; ftask = newTaskFor(task); //注意线程池执行的是RunnableFuture，因为这个Future继承Runnable，所以它是可执行的。 execute(ftask); //返回future return ftask; &#125;submit方法也是支持Runnable的。ExecutorService内部会把Runnable转成Callable，只不过Runnable的返回值为null。FutureTask的属性/状态先看下FutureTask的一些内部属性，才好了解它是怎么运行的。1234567891011121314151617181920212223242526272829//重要属性“状态”state定义为volatile，为了在高并发下能够获取到最新的值private volatile int state; //为state定义了7个状态，看名字都挺好了解的。 private static final int NEW = 0; private static final int COMPLETING = 1; //正常结束 private static final int NORMAL = 2; private static final int EXCEPTIONAL = 3; private static final int CANCELLED = 4; private static final int INTERRUPTING = 5; private static final int INTERRUPTED = 6; //状态之间的转换 * Possible state transitions: * NEW -&gt; COMPLETING -&gt; NORMAL * NEW -&gt; COMPLETING -&gt; EXCEPTIONAL * NEW -&gt; CANCELLED * NEW -&gt; INTERRUPTING -&gt; INTERRUPTED */ //用户提交的真正任务 private Callable&lt;V&gt; callable; //返回结果 private Object outcome; // non-volatile, protected by state reads/writes //记录跑任务的那个线程，只有一个在运行。取消时可以中断这个线程的行为。 private volatile Thread runner; /** Treiber stack of waiting threads */ //这个属性比较重要，后面讲的比较多。记录WaitNode链表的头部，volatile。WaitNode链表是等待结果的线程集合，即这个任务还没跑完时，但同时有很多持有这个future的线程调用了get方法获取结果。 private volatile WaitNode waiters;run方法当这个RunnableFuture提交到线程池后，它做了什么。1234567891011121314151617181920212223242526272829303132333435363738public void run() &#123; //为了防止future重复运行，需要判断是NEW状态。 //同时记录runner属性 //这里UNSAFE的CAS操作在JUC里用的比较多，不展开了（我的原则是，不是这个话题的内容不过多展开，保持专注和精简。） if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try &#123; Callable&lt;V&gt; c = callable; if (c != null &amp;&amp; state == NEW) &#123; V result; boolean ran; try &#123; //执行用户的callable result = c.call(); ran = true; &#125; catch (Throwable ex) &#123; result = null; ran = false; //如果callable运行异常了，这里会把这个异常吃掉，然后调用setException方法 setException(ex); &#125; if (ran) //正常结束就是设置结果 set(result); &#125; &#125; finally &#123; // runner must be non-null until state is settled to // prevent concurrent calls to run() runner = null; // state must be re-read after nulling runner to prevent // leaked interrupts int s = state; if (s &gt;= INTERRUPTING) handlePossibleCancellationInterrupt(s); &#125; &#125;值得注意的是，如果原来的callable任务运行异常了，那么在run方法中会直接catch掉，然后在get的时候才抛出来。这么也是为了做错误隔离，为了callable的异常不会影响到future的运行。setException方法12345678910protected void setException(Throwable t) &#123; if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; //把结果设为异常 outcome = t; //更新状态为EXCEPTIONAL UNSAFE.putOrderedInt(this, stateOffset, EXCEPTIONAL); // final state //finishCompletion作为一个结束动作 finishCompletion(); &#125; &#125;set方法12345678910protected void set(V v) &#123; if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) &#123; //设置结果 outcome = v; //更新状态为NORMAL UNSAFE.putOrderedInt(this, stateOffset, NORMAL); //同样调用了finishCompletion方法 finishCompletion(); &#125; &#125;setException方法和set方法都是protected，不能随意调用，不过子类可以改变它的行为。COMPLETING状态？在上面说到的7个状态中有一个COMPLETING的状态，它表示新建NEW和正常结束NORMAL或异常结束EXCEPTIONAL中间的这么一个状态，在set和setException用到了，会先把NEW状态更新为COMPLETING，再把COMPLETING更新为对应的结果状态。刚开始我认为这个状态是没必要的。因为这个FutureTask只会有一个线程在运行它，不存在竞争，而且看代码也知道，作者没对竞争失败做处理，那么set和setException的CAS操作是肯定会成功的，所以我觉得把COMPLETING变成NEW也是可以的。但是细想如果直接把1UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)变成1UNSAFE.compareAndSwapInt(this, stateOffset, NEW, NORMAL)但是后面还有一步赋值操作。1outcome = v并发下，这时如果有其他线程在CAS后想获取结果，就返回null了。finishCompletion方法1234567891011121314151617181920212223242526272829//如果这个future正常结束，异常结束，被取消了，都会调用这个方法。private void finishCompletion() &#123; // assert state &gt; COMPLETING; //这里会不停的拿头部节点做遍历，直到头部节点为null。这是为了防止在并发下有新的节点新插入进来。 for (WaitNode q; (q = waiters) != null;) &#123; //CAS把WaitNode链表的头部设为null。 if (UNSAFE.compareAndSwapObject(this, waitersOffset, q, null)) &#123; for (;;) &#123; Thread t = q.thread; if (t != null) &#123; q.thread = null; //唤醒WaitNode的线程，唤醒的线程继续在awaitDone方法里做循环。 LockSupport.unpark(t); &#125; WaitNode next = q.next; //直到next为null，完成链表的遍历 if (next == null) break; q.next = null; // unlink to help gc q = next; &#125; break; &#125; &#125; //这个done方法是预留方法，子类可以继承它来做点别的。 done(); callable = null; // to reduce footprint &#125;get方法get方法是重点。因为作者设计FutureTask是支持高并发的，而且用了Lock-Free无锁算法，所以阅读起来会比较费劲。123456public V get() throws InterruptedException, ExecutionException &#123; int s = state; if (s &lt;= COMPLETING) s = awaitDone(false, 0L); return report(s); &#125;WaitNode链表继续看下去之前，先看看WaitNode的定义。很简单，只有一个Thread和next指针。Thread就是指当前需要获取future结果的那个线程。WaitNode通过next指针形成一条链表。12345static final class WaitNode &#123; volatile Thread thread; volatile WaitNode next; WaitNode() &#123; thread = Thread.currentThread(); &#125; &#125;这是在Lock-Free中常见的数据结构，看上去是不是有点像AQS呢？1234567891011/* * Revision notes: This differs from previous versions of this * class that relied on AbstractQueuedSynchronizer, mainly to * avoid surprising users about retaining interrupt status during * cancellation races. Sync control in the current design relies * on a &quot;state&quot; field updated via CAS to track completion, along * with a simple Treiber stack to hold waiting threads. * * Style note: As usual, we bypass overhead of using * AtomicXFieldUpdaters and instead directly use Unsafe intrinsics. */实际上，官方也说了，之前版本的实现是用了AQS的，原因是…（这个原因我不是很懂是啥意思），现在改为Treiber stack算法了。awaitDone方法123456789101112131415161718192021222324252627282930313233343536373839404142434445private int awaitDone(boolean timed, long nanos) throws InterruptedException &#123; final long deadline = timed ? System.nanoTime() + nanos : 0L; WaitNode q = null; boolean queued = false; //死循环直到s &gt; COMPLETING或者超时，当然这个不是真的死循环，大部分情况下线程是会挂起的。 for (;;) &#123; //如果线程是被中断了，则从链表移除当前节点，然后抛异常 if (Thread.interrupted()) &#123; removeWaiter(q); throw new InterruptedException(); &#125; int s = state; //从上面7个状态看出，当s &gt; COMPLETING都是结束的状态，要不正常结束，异常，取消等。可见合理的状态值设计带来的方便。 if (s &gt; COMPLETING) &#123; if (q != null) q.thread = null; return s; &#125; //这里就是为了防止上面我说的，结果赋值时并发下其他线程获取不到值的情况，所以让这个线程yield一下，再做一次循环，说不定下次就是s &gt; COMPLETING呢。 else if (s == COMPLETING) // cannot time out yet Thread.yield(); else if (q == null) //新建一个WaitNode，准备进链表 q = new WaitNode(); else if (!queued) //CAS把WaitNode节点插入到链表的头部，如果失败则下次继续插入 queued = UNSAFE.compareAndSwapObject(this, waitersOffset, q.next = waiters, q); //如果get方法设置了超时时间，则会进入这个分支，如果超时了，也会返回state。还没超时则挂起，挂起的时间为时间差。 else if (timed) &#123; nanos = deadline - System.nanoTime(); if (nanos &lt;= 0L) &#123; //超时需要从链表移除当前节点 removeWaiter(q); return state; &#125; LockSupport.parkNanos(this, nanos); &#125; //注意这里，是最后的步骤，即当s &lt; COMPLETING，已经插入链表了，不是超时的情况 else LockSupport.park(this); &#125; &#125;awaitDone方法返回的是state状态的值。要注意if else执行的顺序，先是判断中断状态，其次判断state的完成状态，再是新建节点，然后插入，最后才是挂起。用for循环去尝试当CAS失败的情况。插入节点的只有这个方法，所以我们可以知道，链表的结构如下：removeWaiter方法的作用是当中断或超时时移除当前的WaitNode。这个方法有点不好理解。12345678910111213141516171819202122private void removeWaiter(WaitNode node) &#123; if (node != null) &#123; node.thread = null; retry: for (;;) &#123; // restart on removeWaiter race for (WaitNode pred = null, q = waiters, s; q != null; q = s) &#123; s = q.next; if (q.thread != null) pred = q; else if (pred != null) &#123; pred.next = s; if (pred.thread == null) // check for race continue retry; &#125; else if (!UNSAFE.compareAndSwapObject(this, waitersOffset, q, s)) continue retry; &#125; break; &#125; &#125; &#125;我们画个图，分情况来理解一下如果q.thread != null因为进来时已经直接把node.thread = null，说明q已经不是当前的node，q是其他线程插入进来的node，这时需要把s，q，pred继续往左移动。如果q.thread == null &amp;&amp; pred != null这时可以把pred的next指向s了，即删除了q。但是如果pred.thread == null，说明pred的线程也把它自己的节点删除了（删除节点的情况除了removeWaiter，还有正常获取结果后也会），所以pred已经没用了，需要重新来找到新的pred。如果q.thread == null &amp;&amp; pred == null说明前面的节点都被删除了，已经没用了，把s直接置为头部。report方法比较简单了1234567891011private V report(int s) throws ExecutionException &#123; Object x = outcome; if (s == NORMAL) //task正常执行就返回结果 return (V)x; if (s &gt;= CANCELLED) //取消则抛异常 throw new CancellationException(); //否则抛出task运行的异常 throw new ExecutionException((Throwable)x); &#125;cancel方法12345678910111213141516171819202122232425//mayInterruptIfRunning参数，取消的同时可以中断runner线程的运行。public boolean cancel(boolean mayInterruptIfRunning) &#123; //状态更新为INTERRUPTING或CANCELLED if (!(state == NEW &amp;&amp; UNSAFE.compareAndSwapInt(this, stateOffset, NEW, mayInterruptIfRunning ? INTERRUPTING : CANCELLED))) return false; try &#123; // in case call to interrupt throws exception //中断 if (mayInterruptIfRunning) &#123; try &#123; Thread t = runner; if (t != null) //调用线程的interrupt方法来中断线程 t.interrupt(); &#125; finally &#123; // final state UNSAFE.putOrderedInt(this, stateOffset, INTERRUPTED); &#125; &#125; &#125; finally &#123; //取消也要把watiers清空掉 finishCompletion(); &#125; return true; &#125;缺点FutureTask有明显的下面两个缺点：重复提交并发会有重复提交的可能，虽然在内部有对状态NEW的判断，但那只是针对那个FutureTask实例的，我们看到，在submit方法中每次提交任务都会new 一个FutureTask出来的。不过现在已经有一个解决方案Memoizer其实很简单，就是用一个key来记录这次的Future，然后放在一个Map里，下次用到时再从Map里取出来。批量任务Future每次只能提交一个任务，而且获取结果之前会一直阻塞，这点也是很不友好的。综上，FutureTask只是提供了一个基本的功能实现，远远不能满足要求高的我们，guava的ListenableFuture和JDK1.8的CompletableFuture都是对Future的增强，前者提供监听器处理结果，后者更加强大，提供链式调用，同步、异步结果返回不同的组合方式来帮助你处理复杂的业务场景。总结源码部分已经介绍的7788了。因为采用了无锁算法，所以实现起来看上去代码比较复杂，看代码时要意识到这个，多想想在高并发下链表会出现怎样的情况，我没有把所有可能出现的情况都罗列出来，所以要靠读者自己多思考。总的来说，Future通过循环判断state状态，挂起、唤醒线程的操作，来实现异步阻塞，通过一个WaitNode链表来处理并发的情况。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>future</tag>
        <tag>futuretask</tag>
        <tag>异步</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TypeToken原理及泛型擦除]]></title>
    <url>%2Fposts%2Fa6e1c381%2F</url>
    <content type="text"><![CDATA[概要借助对TypeToken原理的分析，加强对泛型擦除的理解，使得我们能够知道什么时候，通过什么方式可以获取到泛型的类型。泛型擦除众所周知，Java的泛型只在编译时有效，到了运行时这个泛型类型就会被擦除掉，即List&lt;String&gt;和List&lt;Integer&gt;在运行时其实都是List&lt;Object&gt;类型。为什么选择这种实现机制？不擦除不行么？在Java诞生10年后，才想实现类似于C++模板的概念，即泛型。Java的类库是Java生态中非常宝贵的财富，必须保证向后兼容（即现有的代码和类文件依旧合法）和迁移兼容（泛化的代码和非泛化的代码可互相调用）基于上面这两个背景和考虑，Java设计者采取了“类型擦除”这种折中的实现方式。同时正正有这个这么“坑”的机制，令到我们无法在运行期间随心所欲的获取到泛型参数的具体类型。TypeToken使用使用过Gson的同学都知道在反序列化时需要定义一个TypeToken类型，像这样1234private Type type = new TypeToken&lt;List&lt;Map&lt;String, Foo&gt;&gt;&gt;()&#123;&#125;.getType();//调用fromJson方法时把type传过去，如果type的类型和json保持一致，则可以反序列化出来gson.fromJson(json, type);三个问题为什么要用TypeToken来定义反序列化的类型？正如上面说的，如果直接把List&lt;Map&lt;String, Foo&gt;&gt;的类型传过去，但是因为运行时泛型被擦除了，所以得到的其实是List&lt;Object&gt;，那么后面的Gson就不知道要转成Map&lt;String, Foo&gt;类型了，这时Gson会默认转成LinkedTreeMap类型。为什么带有大括号{}？这个大括号就是精髓所在。大家都知道，在Java语法中，在这个语境，{}是用来定义匿名类，这个匿名类是继承了TypeToken类，它是TypeToken的子类。为什么要通过子类来获取泛型的类型？这是TypeToken能够获取到泛型类型的关键，这是一个巧妙的方法。这个想法是这样子的，既然像List&lt;String&gt;这样中的泛型会被擦除掉，那么我用一个子类SubList extends List&lt;String&gt;这样的话，在JVM内部中会不会把父类泛型的类型给保存下来呢？我这个子类需要继承的父类的泛型都是已经确定了的呀，果然，JVM是有保存这部分信息的，它是保存在子类的Class信息中，具体看：https://stackoverflow.com/questions/937933/where-are-generic-types-stored-in-java-class-files那么我们怎么获取这部分信息呢？还好，Java有提供API出来：123Type mySuperClass = foo.getClass().getGenericSuperclass(); Type type = ((ParameterizedType)mySuperClass).getActualTypeArguments()[0];System.out.println(type);分析一下这段代码，Class类的getGenericSuperClass()方法的注释是：Returns the Type representing the direct superclass of the entity (class, interface, primitive type or void) represented by thisClass.If the superclass is a parameterized type, the Type object returned must accurately reflect the actual type parameters used in the source code. The parameterized type representing the superclass is created if it had not been created before. See the declaration of ParameterizedType for the semantics of the creation process for parameterized types. If thisClass represents either theObject class, an interface, a primitive type, or void, then null is returned. If this object represents an array class then theClass object representing theObject class is returned概括来说就是对于带有泛型的class，返回一个ParameterizedType对象，对于Object、接口和原始类型返回null，对于数 组class则是返回Object.class。ParameterizedType是表示带有泛型参数的类型的Java类型，JDK1.5引入了泛型之 后，Java中所有的Class都实现了Type接口，ParameterizedType则是继承了Type接口，所有包含泛型的Class类都会实现 这个接口。自己调试一下就知道它返回的是什么了。原理核心的方法就是刚刚说的那两句，剩下的就很简单了。我们看看TypeToken的getType方法1234public final Type getType() &#123; //直接返回type return type; &#125;看type的初始化123456789101112131415161718//注意这里用了protected关键字，限制了只有子类才能访问protected TypeToken() &#123; this.type = getSuperclassTypeParameter(getClass()); this.rawType = (Class&lt;? super T&gt;) $Gson$Types.getRawType(type); this.hashCode = type.hashCode(); &#125; //getSuperclassTypeParameter方法 //这几句就是上面的说到 static Type getSuperclassTypeParameter(Class&lt;?&gt; subclass) &#123; Type superclass = subclass.getGenericSuperclass(); if (superclass instanceof Class) &#123; throw new RuntimeException(&quot;Missing type parameter.&quot;); &#125; ParameterizedType parameterized = (ParameterizedType) superclass; //这里注意一下，返回的是Gson自定义的，在$Gson$Types里面定义的TypeImpl等，这个类都是继承Type的。 return $Gson$Types.canonicalize(parameterized.getActualTypeArguments()[0]); &#125;总结在了解原理之后，相信大家都知道怎么去获取泛型的类型了。参考资料https://www.cnblogs.com/doudouxiaoye/p/5688629.html]]></content>
      <categories>
        <category>java</category>
        <category>guava</category>
      </categories>
      <tags>
        <tag>typetoken</tag>
        <tag>泛型</tag>
        <tag>泛型擦除</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ThreadLocal的使用及原理]]></title>
    <url>%2Fposts%2Fd7ec7cff%2F</url>
    <content type="text"><![CDATA[概要如果你还不知道threadlocal，那你就要了解一下，相信你一定会用到它。作用threadlocal最大作用就是提供线程级别的变量生命周期。试想，如果你需要一个变量在一个线程的生命周期内都可以访问到，在不使用threadlocal的前提下你会怎么做？你或许这样做提供一个类级别或者静态变量但是这个方法大家很容易就想到在高并发时会出问题。把这个局部变量一直传递下去但是如果你要调用的方法层次很深呢？难道你对每个方法都增加一个参数吗？显然不实际。所以threadlocal就是提供了一个可行的方案，使得这个变量可以随时访问到，并且不会跟其他线程产生冲突。使用threadlocal的使用很简单，就是一个get, set。123456789101112131415161718public class ThreadLocalTest &#123; //定义一个ThreadLocal的变量, 需要指定类型 public static ThreadLocal&lt;String&gt; threadLocal = new InheritableThreadLocal&lt;&gt;(); @Before public void init()&#123; #set值进去 threadLocal.set("test"); &#125; @Test public void test() &#123; //在需要时get出来 System.out.println("threadLocal's value=" + threadLocal.get()); &#125; &#125;实现原理set我们先从set方法入手看看做了手脚。1234567891011121314151617181920212223public void set(T value) &#123; //取出当前线程 Thread t = Thread.currentThread(); //根据当前线程获取ThreadLocalMap。从getMap方法可以看到这个ThreadLocalMap就是保存在Thread对象里面 ThreadLocalMap map = getMap(t); if (map != null） #map已经存在就是set进去 map.set(this, value); else #不存在新建一个map createMap(t, value); &#125; //getMap方法 ThreadLocalMap getMap(Thread t) &#123; return t.threadLocals; &#125; //createMap方法 //注意，这里的this指的是ThreadLocal对象 void createMap(Thread t, T firstValue) &#123; t.threadLocals = new ThreadLocalMap(this, firstValue); &#125;我们再看看ThreadLocalMap的创建及其他方法。ThreadLocalMap是定义在ThreadLocal里的一个静态类。123456789ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123; table = new Entry[INITIAL_CAPACITY]; //通过ThreadLocal的hashCode确定index，这个我们稍后再说 int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); //重点，把ThreadLocal对象作为key存到了ThreadLocalMap的Entry里 table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); &#125;set方法比较简单，跟普通的Map差不多，把key和value set进去。里面还包含了清理key为null的Entry对象的一些操作。1234567891011121314151617181920212223242526272829303132private void set(ThreadLocal&lt;?&gt; key, Object value) &#123; // We don't use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == key) &#123; e.value = value; return; &#125; if (k == null) &#123; replaceStaleEntry(key, value, i); return; &#125; &#125; tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) rehash(); &#125;get1234567891011121314151617public T get() &#123; //取当前线程 Thread t = Thread.currentThread(); //取出线程的ThreadLocalMap，跟上面是一样的 ThreadLocalMap map = getMap(t); if (map != null) &#123; //根据当前对象ThreadLocal取出ThreadLocalMap.Entry ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) &#123; @SuppressWarnings("unchecked") T result = (T)e.value; return result; &#125; &#125; //如果map为null则做初始化，跟上面的createMap差不多 return setInitialValue(); &#125;图解关系可以看出相同的ThreadLocal在不同的线程有不同的值。主要记住ThreadLocal是作为ThreadLocalMap的key，可能开始有点绕，但是慢慢思考，理清它们的关系就行了。两个问题内存泄漏 ？这是一个对ThreadLocal来说老生常谈的问题了。那使用ThreadLocal为什么会导致内存泄漏？还有我们应该怎么去避免？是我们应该关注的两个点。原因首先，我这里假设大家对java的内存回收机制和引用（Reference）有一定的了解。如果不知道，请自行google了。我们先看看ThreadLocalMap的Entry的定义123456789//对key使用了WeakReferencestatic class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; &#123; /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) &#123; super(k); value = v; &#125; &#125;为什么要使用WeakReference？网上很多的说法都说是使用了弱引用就会被GC一句带过，我觉得很多都说得不清不楚的。我通过自己的理解向大家解析一下：其实很简单，从变量的作用域及引用关系的角度出发思考。试想如果一个ThreadLocal定义为一个类实例的变量或者是一个方法内的局部变量，那么当这个类实例被销毁了或方法退出了，在理想的情况下，垃圾回收器应该回收掉这个ThreadLocal是吧，毕竟它的生命周期已经完结了，但是如果这时ThreadLocalMap还是持有这个ThreadLocal的强引用的话，这个ThreadLocal就不会被回收，直到这个ThreadLocalMap被销毁或者这个线程被销毁。说白了，从上面那个图看出，这样的设计导致的结果是这个ThreadLocal的生命周期跟线程的生命周期挂上钩了。同时，这里又会出现另外一种内存泄漏的问题，即使ThreadLocal回收了，但是value没有被回收，还是会导致内存泄漏。但是你没办法把value设置为WeakReference，因为value不是你的，不归你管。如何防止ThreadLocal采用如下解决内存泄漏，看expungeStaleEntry方法。12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * Expunge a stale entry by rehashing any possibly colliding entries * lying between staleSlot and the next null slot. This also expunges * any other stale entries encountered before the trailing null. See * Knuth, Section 6.4 * * @param staleSlot index of slot known to have null key * @return the index of the next null slot after staleSlot * (all between staleSlot and this slot will have been checked * for expunging). */ private int expungeStaleEntry(int staleSlot) &#123; Entry[] tab = table; int len = tab.length; // expunge entry at staleSlot tab[staleSlot].value = null; tab[staleSlot] = null; size--; // Rehash until we encounter null Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) &#123; ThreadLocal&lt;?&gt; k = e.get(); if (k == null) &#123; e.value = null; tab[i] = null; size--; &#125; else &#123; int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) &#123; tab[i] = null; // Unlike Knuth 6.4 Algorithm R, we must scan until // null because multiple entries could have been stale. while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; &#125; &#125; &#125; return i; &#125;在ThreadLocal的源码你会看到很多清洗数据的代码最终都会调用到这个方法。这个方法主要逻辑，简单来说就是当key为null，把value也设置为null，从而让value也被回收。这个方法的触发点有很多，当对ThreadLocal进行set，get，remove等操作时都会。容器(如tomcat，netty)一般都是使用线程池处理用户到请求，此时用ThreadLocal要特别注意内存泄漏的问题，一个请求结束了，处理它的线程也结束，但此时这个线程并没有死掉，它只是归还到了线程池中，这时候应该清理掉属于它的ThreadLocal信息。所以我们使用ThreadLocal一个比较好的习惯是在finally块调用remove方法。hashcode和0x61c88647？既然ThreadLocal用map就避免不了冲突的产生。在ThreadLocalMap的构造方法中，我们可以看到以下代码123//table的下标的计算方式int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1);table[i] = new Entry(firstKey, firstValue);1234567891011121314//threadLocalHashCode的定义private final int threadLocalHashCode = nextHashCode();private static int nextHashCode() &#123; return nextHashCode.getAndAdd(HASH_INCREMENT); &#125; /** * The difference between successively generated hash codes - turns * implicit sequential thread-local IDs into near-optimally spread * multiplicative hash values for power-of-two-sized tables. */ //HASH_INCREMENT的十进制为1640531527 private static final int HASH_INCREMENT = 0x61c88647;从代码可以看出每个ThreadLocal的实例的threadLocalHashCode的差值为0x61c88647这么多，那为什么要这样做呢？这个魔数的选取与斐波那契散列有关，0x61c88647对应的十进制为1640531527。斐波那契散列的乘数可以用(long) ((1L &lt;&lt; 31) * (Math.sqrt(5) - 1))可以得到2654435769，如果把这个值给转为带符号的int，则会得到-1640531527。换句话说(1L &lt;&lt; 32) - (long) ((1L &lt;&lt; 31) * (Math.sqrt(5) - 1))得到的结果就是1640531527也就是0x61c88647。通过理论与实践，当我们用0x61c88647作为魔数累加为每个ThreadLocal分配各自的ID也就是threadLocalHashCode再与2的幂取模，得到的结果分布很均匀。ThreadLocalMap使用的是线性探测法，均匀分布的好处在于很快就能探测到下一个临近的可用slot，从而保证效率。。为了优化效率。简单来说就是在table[]的size为2的次幂情况下，取模会得到均匀分布。一个优化点从上面得知，ThreadLocal的Map可能会产生冲突，解决冲突的办法是线性探测。而Netty的FastThreadLocal的利用了一个自增序号来作为下标，避免了冲突的产生。123456789101112public FastThreadLocal() &#123; index = InternalThreadLocalMap.nextVariableIndex(); &#125; public static int nextVariableIndex() &#123; int index = nextIndex.getAndIncrement(); if (index &lt; 0) &#123; nextIndex.decrementAndGet(); throw new IllegalStateException(&quot;too many thread-local indexed variables&quot;); &#125; return index; &#125;参考资料https://juejin.im/post/5b5ecf9de51d45190a434308]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>threadlocal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[理解RateLimiter]]></title>
    <url>%2Fposts%2Ffe5da4a5%2F</url>
    <content type="text"><![CDATA[概要为了对系统资源的保护或者在网关限制流量，我们一般用到限流算法。Google开源工具包Guava提供了限流工具类RateLimiter，该类基于令牌桶算法实现流量限制，使用十分方便。RateLimiter原理分析令牌桶算法令牌桶算法的原理是系统会以一个恒定的速度往桶里放入令牌，而如果请求需要被处理，则需要先从桶里获取一个令牌，当桶里没有令牌可取时，则拒绝服务。实现原理RateLimiter有两种限流模式，一种为稳定模式(SmoothBursty:令牌生成速度恒定)，一种为渐进模式(SmoothWarmingUp:令牌生成速度缓慢提升直到维持在一个稳定值)。以下代码基于基本 guava:26.0-jre。看看关键类的继承图SmoothBursty限流效果先看看效果，对这个工具有一个感性认识RateLimiter.create(5.0) 表示每秒产生5个令牌。输出的意思是这次获取令牌所需要等待的时间。属性继承自SmoothRateLimiter的有以下属性1234567891011121314151617/** The currently stored permits. */ double storedPermits; /** The maximum number of stored permits. */ double maxPermits; /** * The interval between two unit requests, at our stable rate. E.g., a stable rate of 5 permits * per second has a stable interval of 200ms. */ double stableIntervalMicros; /** * The time when the next request (no matter its size) will be granted. After granting a request, * this is pushed further in the future. Large requests push this further than small requests. */ private long nextFreeTicketMicros = 0L; // could be either in the past or futurestoredPermits - 当前桶里有多少令牌。maxPermits - 桶可以最大存储多少令牌。stableIntervalMicros - 生成一个令牌的间隔，单位微秒。nextFreeTicketMicros - 这个比较难理解，也是关键，意思是下一个请求允许获取到令牌的微秒数。初始化12345678910public static RateLimiter create(double permitsPerSecond) &#123; return create(permitsPerSecond, SleepingStopwatch.createFromSystemTimer()); &#125; @VisibleForTesting static RateLimiter create(double permitsPerSecond, SleepingStopwatch stopwatch) &#123; RateLimiter rateLimiter = new SmoothBursty(stopwatch, 1.0 /* maxBurstSeconds */); rateLimiter.setRate(permitsPerSecond); return rateLimiter; &#125;SmoothBursty的两个构造参数，一个是stopwatch，这个类的作用是能够获取从初始化时到现在的时间，另一个参数 maxBurstSeconds是 hard code 为 1。123456789101112131415161718public final void setRate(double permitsPerSecond) &#123; checkArgument( permitsPerSecond &gt; 0.0 &amp;&amp; !Double.isNaN(permitsPerSecond), "rate must be positive"); //这里用了 synchronized 锁，锁的范围是这个 rateLimiter 实例。 synchronized (mutex()) &#123; doSetRate(permitsPerSecond, stopwatch.readMicros()); &#125; &#125; @Override final void doSetRate(double permitsPerSecond, long nowMicros) &#123; //resync 方法，它的作用是计算 storedPermits，等下会讲到； resync(nowMicros); //计算 stableIntervalMicros，单位是微秒，用1秒 / 入参的令牌数，意思就是每多少微秒生成一个令牌； double stableIntervalMicros = SECONDS.toMicros(1L) / permitsPerSecond; this.stableIntervalMicros = stableIntervalMicros; doSetRate(permitsPerSecond, stableIntervalMicros); &#125;setRate方法用来初始化令牌生成速率；123456789101112131415@Override void doSetRate(double permitsPerSecond, double stableIntervalMicros) &#123; double oldMaxPermits = this.maxPermits; //这里涉及一个重要的属性 maxPermits，它表示桶最大的存储令牌的数量，注意maxBurstSeconds hard code为1 maxPermits = maxBurstSeconds * permitsPerSecond; if (oldMaxPermits == Double.POSITIVE_INFINITY) &#123; // if we don't special-case this, we would get storedPermits == NaN, below storedPermits = maxPermits; &#125; else &#123; storedPermits = (oldMaxPermits == 0.0) ? 0.0 // initial state : storedPermits * maxPermits / oldMaxPermits; &#125; &#125;doSetRate 是模版方法，我们先看 SmoothBursty 的，等下讲到 SmoothWarmingUp 时会讲它的 doSetRate。这个方法有两个地方用到，一是初始化时，二是调用 RateLimiter 的实例方法 setRate 动态调整速率时。延迟计算初始化就这么简单了。可能有人在想既然是令牌桶算法，应该有个类似定时器的东东来持续往桶放令牌才对啊，我刚开始也是这么想的，看了代码觉得自己还是太嫩了，如果开启一个定时器无可厚非，但如果系统需要N个不同速率的桶来针对不同的场景或用户，就会极大的消耗系统资源。RateLimiter用了一种类似于延迟计算的方法，把桶里令牌数量的计算放在下一个请求中计算，即桶里的令牌数 storedPermits 不是实时更新的，而是等到下一个请求过来时才更新的，具体我们来看看消费令牌的过程。获取令牌acquire主要有两个方法，一是 acquire，一是 tryAcquire。区别是如果桶里没有令牌，前者会阻塞，后者会直接返回 false。我们先看看 acquire 方法12345678910111213141516171819202122@CanIgnoreReturnValue //这个方法主要是获取令牌的同时，返回需要等待的时间，主要就是reserve方法，至于 stopwatch.sleepMicrosUninterruptibly 大家理解为 sleep 就好了。 public double acquire(int permits) &#123; long microsToWait = reserve(permits); stopwatch.sleepMicrosUninterruptibly(microsToWait); return 1.0 * microsToWait / SECONDS.toMicros(1L); &#125; final long reserve(int permits) &#123; checkPermits(permits //这里用 synchronized 锁，所以下面的逻辑大家不用考虑由并发产生的问题； synchronized (mutex()) &#123; //stopwatch.readMicros() 的作用是获取从初始化到现在的系统时间微秒数。 return reserveAndGetWaitLength(permits, stopwatch.readMicros()); &#125; &#125; //获取令牌并等待 final long reserveAndGetWaitLength(int permits, long nowMicros) &#123; long momentAvailable = reserveEarliestAvailable(permits, nowMicros); return max(momentAvailable - nowMicros, 0); &#125;reserveEarliestAvailable 是整个 RateLimiter 的核心方法，它是 SmoothRateLimite 的一个模板方法。123456789101112131415161718@Override final long reserveEarliestAvailable(int requiredPermits, long nowMicros) &#123; resync(nowMicros); long returnValue = nextFreeTicketMicros; //storedPermitsToSpend 是可以消费的令牌数，最多也就取 storedPermits 这么多了； double storedPermitsToSpend = min(requiredPermits, this.storedPermits); //freshPermits 字面意思新鲜的令牌，我们理解为还没生成的或者将来会生成的令牌。假如我要10个令牌，但是桶里现在只有5个令牌，那么 freshPermits 值为 5 = 10 - 5； double freshPermits = requiredPermits - storedPermitsToSpend; //然后就通过 freshPermits 计算出需要等待的时间 waitMicros。storedPermitsToWaitTime 是一个模板方法，对 SmoothBursty 来说这个没啥用，它始终返回 0，所以 waitMicros = freshPermits * stableIntervalMicros； long waitMicros = storedPermitsToWaitTime(this.storedPermits, storedPermitsToSpend) + (long) (freshPermits * stableIntervalMicros); //更新 nextFreeTicketMicros，追加 waitMicros； this.nextFreeTicketMicros = LongMath.saturatedAdd(nextFreeTicketMicros, waitMicros); //storedPermits 追减用掉的令牌； this.storedPermits -= storedPermitsToSpend; return returnValue; &#125;reserveEarliestAvailable的返回值，注意了，这里返回的是更新前的 nextFreeTicketMicros，也就是上一个请求更新的 nextFreeTicketMicros。那么这个 waitMicros 等待时间也不是当前请求需要等待的时间，而是下一个请求需要等待的时间，这个涉及到 RateLimiter 一个很重要的设计理念，就是“预消费”，通俗点理解即“前人消费，后人买单”，理解好这点，是使用和理解 RateLimiter 的关键。我举一个例子来助于理解，桶的速率为每秒产生5个令牌，现在桶里有4个令牌，现在过来一个请求需要10个令牌，那么这个请求会被无阻塞允许，不需要等待，同时又过来一个请求，现在桶里已经没有令牌了，而且上一个请求还“欠下”6个令牌，那么这个请求需要等待 (10 - 4) / 5 秒的时间，才被允许执行。1234567891011121314151617//刚刚说的延迟计算令牌数就在这里。这个方法是用来计算 storedPermits （桶里的令牌数），nowMicros 是当前的微秒数，nextFreeTicketMicros 上面说过了。void resync(long nowMicros) &#123; // if nextFreeTicket is in the past, resync to now if (nowMicros &gt; nextFreeTicketMicros) &#123; double newPermits = (nowMicros - nextFreeTicketMicros) / coolDownIntervalMicros(); //令牌数不能超过 maxPermits； storedPermits = min(maxPermits, storedPermits + newPermits); //把 nextFreeTicketMicros 置为当前时间。 nextFreeTicketMicros = nowMicros; &#125; &#125; //coolDownIntervalMicros 是一个模板方法，看 SmoothBursty 的，值等于 stableIntervalMicros @Override double coolDownIntervalMicros() &#123; return stableIntervalMicros; &#125;所以resync的意思就是如果当前时间大于 nextFreeTicketMicros，就用当前时间 - nextFreeTicketMicros / 每 stableIntervalMicros 生成一个令牌，即这个时间差可以生成多少个令牌；我用一个图来表示会更加清晰为什么要“预消费”RateLimiter 它是这样想的：Last, but not least: consider a RateLimiter with rate of 1 permit per second, currently completely unused, and an expensive acquire(100) request comes. It would be nonsensical to just wait for 100 seconds, and /then/ start the actual task. Why wait without doing anything? A much better approach is to /allow/ the request right away (as if it was an acquire(1) request instead), and postpone /subsequent/ requests as needed. In this version, we allow starting the task immediately, and postpone by 100 seconds future requests, thus we allow for work to get done in the meantime instead of waiting idly.大概意思是，假设令牌产生的速率为1秒一个，系统平时是很空闲的，突然来了一个 expensive acquire(100) 的请求，难道我要瞎等100秒才执行吗？这毫无意义，不能充分利用资源啊，所以干脆可以直接允许好了，不要做无谓的等待。简单来说就是为了突发性。消费场景分析我们分情况分析一下就清楚了：nowMicros &gt; nextFreeTicketMicros这种场景发生在刚初始化时，或者桶里的令牌还有剩余。如果请求所需令牌 &lt; 桶里的即桶里令牌满足这次消费的话，那么 nextFreeTicketMicros 会移动到 nowMicros 的位置令牌数 storedPermits = 原来 - 消费的 + 这段时间增加的。如果请求所需令牌 &gt;= 桶里的这时会优先把桶里的令牌全部拿走，那么 storedPermits 就等于0了。如果还不够，就会发生预消费，那么 nextFreeTicketMicros 会后移，移动多少？就是需要产生“溢出”令牌数的时间。nowMicros &lt; nextFreeTicketMicros在上面有一个场景 nextFreeTicketMicros 会后移，移动了多少不知道，要看上一个请求，那么如果这段时间内有请求过来呢？这时当前的请求就要为上一个请求“买单”了，它需要等待到 nextFreeTicketMicros 这个时刻才能允许执行，但此时桶里令牌数是 0 的，所以这个请求也是会预消费的。SmoothWarmingUpSmoothBursty 是以一个固定的速率来产生令牌的，它具有突发性，这个可能适用大多数场景。而 SmoothWarmingUp 考虑的是譬如一个系统刚启动，但如果这时有大量请求过来，因为突发性，这些请求都会被允许，但此时系统可能没有那么多资源去响应，所以需要一个“热身”时间，SmoothWarmingUp 就派上用场了。它跟 SmoothBursty 的大概思路都是差不多的，只是个别地方有差别，主要就是之前提到几个模板方法，我们来看看。限流效果SmoothWarmingUp 的效果是刚开始产生令牌的速率比较慢，随着请求过来，会进入“热身”期，速率逐渐提升到 permitsPerSecond 这个速度；但是如果没有请求了，又会“冷却”下去，请求过来又要从“热身”开始。初始化初始化也是调用 create，不过参数列表有点不同12345678910111213141516171819202122232425262728//permitsPerSecond 是“热身”后的稳定速率； //warmupPeriod 是“热身”时间，如果这段时间内持续有请求过来消费令牌，就会达到一个稳定的速率，这时跟 SmoothBursty 效果一样； //unit 是 warmupPeriod 的单位； public static RateLimiter create(double permitsPerSecond, long warmupPeriod, TimeUnit unit) &#123; checkArgument(warmupPeriod &gt;= 0, "warmupPeriod must not be negative: %s", warmupPeriod); //coldFactor hard code 为 3； return create( permitsPerSecond, warmupPeriod, unit, 3.0, SleepingStopwatch.createFromSystemTimer()); &#125; @VisibleForTesting static RateLimiter create( double permitsPerSecond, long warmupPeriod, TimeUnit unit, double coldFactor, SleepingStopwatch stopwatch) &#123; RateLimiter rateLimiter = new SmoothWarmingUp(stopwatch, warmupPeriod, unit, coldFactor); rateLimiter.setRate(permitsPerSecond); return rateLimiter; &#125; SmoothWarmingUp( SleepingStopwatch stopwatch, long warmupPeriod, TimeUnit timeUnit, double coldFactor) &#123; super(stopwatch); this.warmupPeriodMicros = timeUnit.toMicros(warmupPeriod); this.coldFactor = coldFactor; &#125;“热身”速率函数及说明由于接下来涉及到一些计算，我们先看看“热身”函数的定义及图像123456789101112131415161718192021222324252627282930313233343536373839/** * This implements the following function where coldInterval = coldFactor * stableInterval. * * &lt;pre&gt; * ^ throttling * | * cold + / * interval | /. * | / . * | / . ← &quot;warmup period&quot; is the area of the trapezoid between * | / . thresholdPermits and maxPermits * | / . * | / . * | / . * stable +----------/ WARM . * interval | . UP . * | . PERIOD. * | . . * 0 +----------+-------+--------------→ storedPermits * 0 thresholdPermits maxPermits * &lt;/pre&gt; * * Before going into the details of this particular function, let&apos;s keep in mind the basics: * * &lt;ol&gt; * &lt;li&gt;The state of the RateLimiter (storedPermits) is a vertical line in this figure. * &lt;li&gt;When the RateLimiter is not used, this goes right (up to maxPermits) * &lt;li&gt;When the RateLimiter is used, this goes left (down to zero), since if we have * storedPermits, we serve from those first * &lt;li&gt;When _unused_, we go right at a constant rate! The rate at which we move to the right is * chosen as maxPermits / warmupPeriod. This ensures that the time it takes to go from 0 to * maxPermits is equal to warmupPeriod. * &lt;li&gt;When _used_, the time it takes, as explained in the introductory class note, is equal to * the integral of our function, between X permits and X-K permits, assuming we want to * spend K saved permits. * &lt;/ol&gt; * * &lt;p&gt;In summary, the time it takes to move to the left (spend K permits), is equal to the area of * the function of width == K.首先不要被吓到，还是很简单的，我来说明一下。x 轴是 storedPermits，即桶里的令牌数。轴上主要刻有两个值，一是thresholdPermits，这个等下会讲到；一个是maxPermits；y 轴是生成一个令牌的间隔，单位微秒。轴上主要刻有两个值，一是stable interval；一个是 cold interval，coldInterval = coldFactor * stableInterval，由于 coldFactor hard code 为 3，所以 coldInterval 等于3倍的 stable interval。warmup period 是入参的“热身”时间。由这几个值构成的左边的长方形和右边的梯形。由于 x 轴是令牌数，y 轴是生成令牌的间隔，所以它们的乘积是一个时间。doSetRate方法1234567891011121314151617181920212223@Override void doSetRate(double permitsPerSecond, double stableIntervalMicros) &#123; double oldMaxPermits = maxPermits; //coldIntervalMicros - 固定 stableIntervalMicros * 3，这里的stableIntervalMicros跟SmoothBursty一样。 double coldIntervalMicros = stableIntervalMicros * coldFactor; //thresholdPermits - 桶里令牌数的阈值，低于这个值之后就会进入稳定速率期；但高于这个值，又会回到“热身”期。 thresholdPermits = 0.5 * warmupPeriodMicros / stableIntervalMicros; //maxPermits - 意思也是桶里允许最多的令牌 maxPermits = thresholdPermits + 2.0 * warmupPeriodMicros / (stableIntervalMicros + coldIntervalMicros); //slope - 斜率，就是你在图形上看到那条斜线的斜率，这时用来方便已知 storedPermits 时，求出当前的 coldIntervalMicros。条件都已知了，斜率的计算不多说。 slope = (coldIntervalMicros - stableIntervalMicros) / (maxPermits - thresholdPermits); if (oldMaxPermits == Double.POSITIVE_INFINITY) &#123; // if we don't special-case this, we would get storedPermits == NaN, below storedPermits = 0.0; &#125; else &#123; //还有一点注意，这里初始化时，桶里的令牌数为满，跟 SmoothBursty 不一样。 storedPermits = (oldMaxPermits == 0.0) ? maxPermits // initial state is cold : storedPermits * maxPermits / oldMaxPermits; &#125; &#125;这里对几个参数的计算说明一下：thresholdPermits为什么 thresholdPermits = 0.5 * warmupPeriodMicros / stableIntervalMicros？先看看官方的注释Assuming we have saturated demand, the time to go from maxPermits to thresholdPermits isequal to warmupPeriod. And the time to go from thresholdPermits to 0 is warmupPeriod/2. (Thereason that this is warmupPeriod/2 is to maintain the behavior of the original implementationwhere coldFactor was hard coded as 3.)根据官方的注释，说“热身”的时间是稳定时间的2倍（我这里表述不准确），即梯形面积为长方形面积的2倍，要保持跟 coldFactor 写死为3一样，原因是希望令牌速率提升的幅度跟它所需要的时间的比例保持一致（这点我不知道理解的对不对，希望有人帮我佐证）因为梯形面积是已知的，又知道长方形的面积和一条边长，容易求得 thresholdPermits。maxPermits为什么 maxPermits = thresholdPermits + 2.0 * warmupPeriodMicros / (stableIntervalMicros + coldIntervalMicros) ？这个简单，利用梯形面积公式求出高，然后再加上 thresholdPermits。消费令牌的主要逻辑在 reserveEarliestAvailable 方法，里面有一个模板方法 storedPermitsToWaitTime，我们看看 SmoothWarmingUp 的实现。1234567891011121314151617181920212223242526272829@Override long storedPermitsToWaitTime(double storedPermits, double permitsToTake) &#123; //availablePermitsAboveThreshold 表示多于 thresholdPermits 的可用令牌数； double availablePermitsAboveThreshold = storedPermits - thresholdPermits; long micros = 0; // measuring the integral on the right part of the function (the climbing line) //如果 availablePermitsAboveThreshold &gt; 0，说明还在“热身”期，令牌的数量需要控制在 thresholdPermits； if (availablePermitsAboveThreshold &gt; 0.0) &#123; //permitsAboveThresholdToTake 表示这次允许取的最大的令牌数； double permitsAboveThresholdToTake = min(availablePermitsAboveThreshold, permitsToTake); // TODO(cpovirk): Figure out a good name for this variable. //length 表示在大梯形中，以 permitsToTime(availablePermitsAboveThreshold) 为右边的底部，以 permitsToTime(availablePermitsAboveThreshold - permitsAboveThresholdToTake) 为左边的底部，构成的小梯形中，这两条边的和，用于下面的计算； double length = permitsToTime(availablePermitsAboveThreshold) + permitsToTime(availablePermitsAboveThreshold - permitsAboveThresholdToTake); //micros 就是计算小梯形的面积了，permitsAboveThresholdToTake 就是这个小梯形的高了； micros = (long) (permitsAboveThresholdToTake * length / 2.0); permitsToTake -= permitsAboveThresholdToTake; &#125; // measuring the integral on the left part of the function (the horizontal line) //如果多于 thresholdPermits 的令牌数不够，那么就会进入稳定期，使用稳定的速率。 micros += (long) (stableIntervalMicros * permitsToTake); return micros; &#125; //permitsToTime 就是利用斜率求出 y 轴的值。 private double permitsToTime(double permits) &#123; return stableIntervalMicros + permits * slope; &#125;看一下图像就清楚了从 storedPermitsToWaitTime 看出，SmoothWarmingUp 会优先取出超过 thresholdPermits 的令牌，但即使有令牌可用，还是会阻塞请求，以这样来防止启动时的突发性。随着请求增加，令牌的减少，桶的令牌会达到 thresholdPermits，这时就相当于“热身”完了，跟 SmoothBursty 一样。但如果一直没有请求来消费令牌，令牌数增加，就会从新进去“热身”期了。coolDownIntervalMicros在 resync 方法中，还有一个模板方法 coolDownIntervalMicros，在 SmoothWarmingUp 的实现中为1234@Override double coolDownIntervalMicros() &#123; return warmupPeriodMicros / maxPermits; &#125;这个方法是用于得出从上一个请求到当请求的时间内，可以生成令牌的时间间隔，在 SmoothBursty 的实现中它就是 stableIntervalMicros。但在这里我不明白为什么要这样计算（梯形面积 / maxPermits 得出是什么？？？），如果有人知道，希望你留言告知我这个数学渣。setRate的公平性考虑RateLimiter 可以动态调整产生令牌的速率，但是这里涉及一个问题，如何处理当前被阻塞的请求以及后续请求？先看看官方的注释:1234567891011121314/** * Updates the stable rate of this &#123;@code RateLimiter&#125;, that is, the &#123;@code permitsPerSecond&#125; * argument provided in the factory method that constructed the &#123;@code RateLimiter&#125;. Currently * throttled threads will &lt;b&gt;not&lt;/b&gt; be awakened as a result of this invocation, thus they do not * observe the new rate; only subsequent requests will. * * &lt;p&gt;Note though that, since each request repays (by waiting, if necessary) the cost of the * &lt;i&gt;previous&lt;/i&gt; request, this means that the very next request after an invocation to &#123;@code * setRate&#125; will not be affected by the new rate; it will pay the cost of the previous request, * which is in terms of the previous rate. * * &lt;p&gt;The behavior of the &#123;@code RateLimiter&#125; is not modified in any other way, e.g. if the &#123;@code * RateLimiter&#125; was configured with a warmup period of 20 seconds, it still has a warmup period of * 20 seconds after this method invocation.注释的意思说了当前被阻塞的线程不会因此醒过来，它们对速率的改变没有感知，接下来的请求才会适应新的速率。Note though that, since each request repays (by waiting, if necessary) the cost of the previous request, this means that the very next request after an invocation to {@codesetRate} will not be affected by the new rate; it will pay the cost of the previous request, which is in terms of the previous rate.其中这句话不好理解，我的理解是，假设速率降低了，如果需要对当前被阻塞的请求做调整的话，那么它们的阻塞时间会增加（这里假设的结果是增加），由于连锁反应，最后导致 nextFreeTicketMicros 会后移，这就对于改变速率后的请求不公平了。所以 RateLimiter 的做法是当前阻塞的请求还是按照原来时间等待，后续的请求用新的速率，这样实现也比较简单，对后续的请求也公平。tryAcquire补充说明一下tryAcquire，这方法实际应用比acquire 方法还要实用。12345678910111213141516171819202122232425public boolean tryAcquire(int permits, long timeout, TimeUnit unit) &#123; long timeoutMicros = max(unit.toMicros(timeout), 0); checkPermits(permits); long microsToWait; synchronized (mutex()) &#123; long nowMicros = stopwatch.readMicros(); if (!canAcquire(nowMicros, timeoutMicros)) &#123; return false; &#125; else &#123; microsToWait = reserveAndGetWaitLength(permits, nowMicros); &#125; &#125; stopwatch.sleepMicrosUninterruptibly(microsToWait); return true; &#125; //判断就是 canAcquire 方法，很简单，就是判断 nextFreeTicketMicros 的位置，因为你最多也就需要等待到 nextFreeTicketMicros 这么长的时间嘛。 private boolean canAcquire(long nowMicros, long timeoutMicros) &#123; return queryEarliestAvailable(nowMicros) - timeoutMicros &lt;= nowMicros; &#125; @Override final long queryEarliestAvailable(long nowMicros) &#123; return nextFreeTicketMicros; &#125;tryAcquire 会先去判断是否能够在 timeout 的等待时间内能够获取到令牌，如果可以就阻塞等待，如果不能则直接返回false。总结Guava 的 RateLimiter 是一个高效低耗，简单易用，优秀的限流工具，它基于令牌桶算法，并且提供了一个很好的实现参考。参考资料https://blog.wangqi.love/articles/Java/Guava%20RateLimiter%E5%88%86%E6%9E%90.htmlhttps://segmentfault.com/a/1190000012875897https://www.jianshu.com/p/3dfae5c15eb9]]></content>
      <categories>
        <category>java</category>
        <category>guava</category>
      </categories>
      <tags>
        <tag>guava</tag>
        <tag>ratelimiter</tag>
        <tag>限流</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DelayQueue实现原理]]></title>
    <url>%2Fposts%2F255bd548%2F</url>
    <content type="text"><![CDATA[概要任务调度和缓存框架的都会用到DelayQueu作为底层实现，了解它可以让我们更好理解这些框架的本质。背景如果要判断一个缓存对象超时没有，一种笨笨的办法就是，使用一个后台线程，遍历所有对象，挨个检查。这种笨笨的办法简单好用，但是对象数量过多时，可能存在性能问题，检查间隔时间不好设置，间隔时间过大，影响精确度，多小则存在效率问题。而且做不到按超时的时间顺序处理。那么DelayQueue就是用来解决这类问题。实现原理如果看过 PriorityQueue 的源码，就会发现 DelayQueue 的源码实现起来很简答，基本都是调用 PriorityQueue 的插入和取出。不过 DelayQueue 支持高并发，即每个方法开头和结尾都有用 ReentrantLock。take方法我们重点来看看 take 方法：12345678910111213141516171819202122232425262728293031323334353637383940/** * Retrieves and removes the head of this queue, waiting if necessary * until an element with an expired delay is available on this queue. * * @return the head of this queue * @throws InterruptedException &#123;@inheritDoc&#125; */ public E take() throws InterruptedException &#123; final ReentrantLock lock = this.lock; lock.lockInterruptibly(); try &#123; for (;;) &#123; E first = q.peek(); if (first == null) available.await(); else &#123; long delay = first.getDelay(NANOSECONDS); if (delay &lt;= 0) return q.poll(); first = null; // don't retain ref while waiting if (leader != null) available.await(); else &#123; Thread thisThread = Thread.currentThread(); leader = thisThread; try &#123; available.awaitNanos(delay); &#125; finally &#123; if (leader == thisThread) leader = null; &#125; &#125; &#125; &#125; &#125; finally &#123; if (leader == null &amp;&amp; q.peek() != null) available.signal(); lock.unlock(); &#125; &#125;可以看出延迟的实现原理就是用到了 Condition.awaitNanos(delay) 方法。先 peek 看看有没有元素，再看看元素有没有过期，过期就 poll 取出，还没过期就是 await 等待。这里有两点需要注意：leader线程的作用先看看官方注释：1234567891011121314151617/** * Thread designated to wait for the element at the head of * the queue. This variant of the Leader-Follower pattern * (http://www.cs.wustl.edu/~schmidt/POSA/POSA2/) serves to * minimize unnecessary timed waiting. When a thread becomes * the leader, it waits only for the next delay to elapse, but * other threads await indefinitely. The leader thread must * signal some other thread before returning from take() or * poll(...), unless some other thread becomes leader in the * interim. Whenever the head of the queue is replaced with * an element with an earlier expiration time, the leader * field is invalidated by being reset to null, and some * waiting thread, but not necessarily the current leader, is * signalled. So waiting threads must be prepared to acquire * and lose leadership while waiting. */ private Thread leader = null;说了是用到 Leader-Follower 模式。如果一个线程是 leader 线程，那么它只会等待 available.awaitNanos(delay) 这么多时间，其他后来的 follower 线程只能干等。意思就是一定是 leader 线程先取到头元素，其他线程需要等待 leader 线程的唤醒。这样就可以简化竞争的操作，直接让后面的线程等待，把竞争交给 Condition 来做。first == null目的是为了做 GC。假设没有这一句，那么这里很有可能是 follower 线程在等待的过程中一直持有 first 的引用，而 leader 线程已经完成任务了，都把 first 都释放了，原来希望被回收的 first 却一直没有被回收。在极端的情况下，在一瞬间高并发，会有大量的 follower 线程持有 first，而需要等这些线程都会唤醒后，first 才会被释放回收。offer方法offer 方法，add 和 put 最终还是调到 offer 方法。123456789101112131415161718192021/** * Inserts the specified element into this delay queue. * * @param e the element to add * @return &#123;@code true&#125; * @throws NullPointerException if the specified element is null */ public boolean offer(E e) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; q.offer(e); if (q.peek() == e) &#123; leader = null; available.signal(); &#125; return true; &#125; finally &#123; lock.unlock(); &#125; &#125;放入元素，如果插入的元素是放在了头部的话：把 leader 线程置为 null因为 leader 的意义就是想要取头元素的那个线程，那么旧的 leader 将没有意义。唤醒在等待的线程原本线程都在等待头元素，但是头元素改变了，就唤醒一个线程让它重新取出头元素，并成为新的 leader （看 take 方法里面是一个 for 的死循环）。总结无界队列 - 因为本质是PriorityQueue，PriorityQueue会无限扩展；item 需要实现 Delayed 接口，实现 compareTo 和 getDelay 方法，前者用于放入队列时排序，后者用于如果返回小于 0 且在队列头，则可以取出来；注意 getDelay 返回的是 NANOSECONDS；poll 头元素还没过期则会返回 null；重入锁是非公平的；是实现定时任务的关键；关于 compareTo 和 getDelay，我之前有点混淆，compareTo 是决定放到队列的位置，getDelay 是觉得取出来时的延迟时间；compareTo 和 getDelay 是没有关系的，就是说，队列头的元素可能 getDelay 很大，它后面的元素 getDelay 很小，不一定是说 getDelay 小是放在队列前面的；一般实际使用，我们会用使用相同的属性来做 compareTo 和 getDelay，使到它们是一致的。参考资料http://cmsblogs.com/?p=2413https://www.zybuluo.com/mikumikulch/note/712598]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>DelayQueue</tag>
        <tag>延迟队列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PriorityQueue实现原理]]></title>
    <url>%2Fposts%2F49dd368f%2F</url>
    <content type="text"><![CDATA[概要PriorityQueue是一个重要数据结构，是DelayQueue的底层实现，为例如任务调度的实现提供底层的数据结构。PriorityQueue原理分析如果你了解堆排序的话，它的实现对你而言就显得很简单。先看看PriorityQueue有什么属性1234567891011121314151617181920/** * Priority queue represented as a balanced binary heap: the two * children of queue[n] are queue[2*n+1] and queue[2*(n+1)]. The * priority queue is ordered by comparator, or by the elements' * natural ordering, if comparator is null: For each node n in the * heap and each descendant d of n, n &lt;= d. The element with the * lowest value is in queue[0], assuming the queue is nonempty. */ transient Object[] queue; // non-private to simplify nested class access /** * The number of elements in the priority queue. */ private int size = 0; /** * The comparator, or null if priority queue uses elements' * natural ordering. */ private final Comparator&lt;? super E&gt; comparator;主要属性有 queue 一个数组，Comparator 比较器。添加元素的 add/offer 方法：1234567891011121314151617181920212223/** * Inserts the specified element into this priority queue. * * @return &#123;@code true&#125; (as specified by &#123;@link Queue#offer&#125;) * @throws ClassCastException if the specified element cannot be * compared with elements currently in this priority queue * according to the priority queue's ordering * @throws NullPointerException if the specified element is null */ public boolean offer(E e) &#123; if (e == null) throw new NullPointerException(); modCount++; int i = size; if (i &gt;= queue.length) grow(i + 1); size = i + 1; if (i == 0) queue[0] = e; else siftUp(i, e); return true; &#125;加入元素，grow是扩容，元素的插入主要看siftUp。先看看grow方法：12345678910111213141516/** * Increases the capacity of the array. * * @param minCapacity the desired minimum capacity */ private void grow(int minCapacity) &#123; int oldCapacity = queue.length; // Double size if small; else grow by 50% int newCapacity = oldCapacity + ((oldCapacity &lt; 64) ? (oldCapacity + 2) : (oldCapacity &gt;&gt; 1)); // overflow-conscious code if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); queue = Arrays.copyOf(queue, newCapacity); &#125;奇怪的 grow，就如注释所说的，小于 64 时扩容 2 倍，大于 64 时扩容 50%。siftUp方法：1234567891011121314151617181920212223242526272829303132/** * Inserts item x at position k, maintaining heap invariant by * promoting x up the tree until it is greater than or equal to * its parent, or is the root. * * To simplify and speed up coercions and comparisons. the * Comparable and Comparator versions are separated into different * methods that are otherwise identical. (Similarly for siftDown.) * * @param k the position to fill * @param x the item to insert */ private void siftUp(int k, E x) &#123; if (comparator != null) siftUpUsingComparator(k, x); else siftUpComparable(k, x); &#125; @SuppressWarnings("unchecked") private void siftUpComparable(int k, E x) &#123; Comparable&lt;? super E&gt; key = (Comparable&lt;? super E&gt;) x; while (k &gt; 0) &#123; int parent = (k - 1) &gt;&gt;&gt; 1; Object e = queue[parent]; if (key.compareTo((E) e) &gt;= 0) break; queue[k] = e; k = parent; &#125; queue[k] = key; &#125;siftUp 方法，为什么叫 up 呢，因为插入的位置是数组的最后，也就是二叉树的最后一个节点，所以要向上调整，这里就涉及堆排序的调整。comparator 为空时，用 compareTo 比较，直到 parent 比插入的元素大，否则交换，就是这么简单。取出元素的poll方法：123456789101112public E poll() &#123; if (size == 0) return null; int s = --size; modCount++; E result = (E) queue[0]; E x = (E) queue[s]; queue[s] = null; if (s != 0) siftDown(0, x); return result; &#125;取出元素，然后把最后的元素放在第一个的位置，然后调整，本质也是涉及堆排序的调整。总结利用堆排序实现插入、取出等操作时的重排序，目的是效率较高（我一开始认为是一个线性 array，然后通过 shift - 类似插入排序的样子，但是想想这是个 O(n) 的操作，堆排序是 O(logn)快多了）；默认实现是最小堆，当然也可以传入相反比较结果的 Comparator 实现最大堆；没有传入 Comparator 的话，默认使用元素的 compareTo 方法，所以元素要是 Comparable 的，不然会报错；全程没有锁，不支持高并发，不过有PriorityBlockingQueue；不允许null元素；参考资料https://www.cnblogs.com/CarpenterLee/p/5488070.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>PriorityQueue</tag>
        <tag>优先级队列</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[堆排序]]></title>
    <url>%2Fposts%2Fc2a5fdc5%2F</url>
    <content type="text"><![CDATA[概要堆排序是一种重要数据结构+算法，一般作为优先级队列的底层数据结构，对它的理解有助于我们更好，更快速的对上层工具的使用。堆排序原理堆排序算法介绍堆是一种重要的数据结构，为一棵完全二叉树, 底层如果用数组存储数据的话，假设某个元素为序号为i(Java数组从0开始,i为0到n-1)。如果它有左子树，那么左子树的位置是2i+1；如果有右子树，右子树的位置是2i+2；如果有父节点，父节点的位置是(n-1)/2取整；分为最大堆和最小堆，最大堆的任意子树根节点不小于任意子结点，最小堆的根节点不大于任意子结点。所谓堆排序就是利用堆这种数据结构来对数组排序，我们使用的是最大堆。处理的思想和冒泡排序，选择排序非常的类似，一层层封顶，只是最大元素的选取使用了最大堆。最大堆的最大元素一定在第0位置，构建好堆之后，交换0位置元素与顶即可。堆排序为原位排序(空间小), 且最坏运行时间是O(nlgn)，是渐进最优的比较排序算法。堆排序的条件1、是一棵完全二叉树（除了最后一层之外的其他每一层都被完全填充，并且所有结点都保持向左对齐）2、最大堆要求节点的元素都要不小于其孩子，最小堆要求节点元素都不大于其左右孩子构造堆的过程（以最大堆为例）假设初始数组为 [1,2,3,4,5,6,7]1、从 array.length / 2 开始，即节点42、节点4没有子节点，结束3、到节点3，3和孩子6和7比较，7比较大，和3交换，变成了图24、到节点2，2和5交换，变成了图35、到节点1，1和7交换后，破坏了原来 7 -&gt; 6, 3 的顺序，需要继续调整，于是1和6交换，变成了图4，结束堆排序的步骤（以最大堆为例）1、对数组 n 个元素构建最大堆 （就是上面的过程）2、将堆顶最大值和数组最后的元素进行替换3、由于步骤2的的交换可能破环了最大堆的性质，第0个不再是最大元素，就对当前元素进行调整，调整的方法跟上面说的是一样的，最终的结果会得到一个最大堆。代码及说明123456789101112131415161718192021222324252627282930313233343536373839404142public static void heapSort(int[] array) &#123; if (array == null || array.length &lt;= 1) &#123; return; &#125; buildMaxHeap(array); for (int i = array.length - 1; i &gt;= 1; i--) &#123; ArrayUtils.exchangeElements(array, 0, i); maxHeap(array, i, 0); &#125; &#125; private static void buildMaxHeap(int[] array) &#123; if (array == null || array.length &lt;= 1) &#123; return; &#125; int half = array.length / 2; for (int i = half; i &gt;= 0; i--) &#123; maxHeap(array, array.length, i); &#125; &#125; private static void maxHeap(int[] array, int heapSize, int index) &#123; int left = index * 2 + 1; int right = index * 2 + 2; int largest = index; if (left &lt; heapSize &amp;&amp; array[left] &gt; array[index]) &#123; largest = left; &#125; if (right &lt; heapSize &amp;&amp; array[right] &gt; array[largest]) &#123; largest = right; &#125; if (index != largest) &#123; ArrayUtils.exchangeElements(array, index, largest); maxHeap(array, heapSize, largest); &#125; &#125;heapSort是入口方法，buildMaxHeap是构建最大堆，maxHeap是每次对节点的调整。可以看出，我们一开始构建堆，从 array.length / 2 开始，直到第0个，这样就把最大堆构建好了。maxHeap是核心算法，它的作用是跟两个子节点比较，如果发现有比它大的，就交换，如果发生交换，就从交换的节点继续调整。总结用一个数组代表一棵完全二叉树：左节点在 2*i + 1 的位置右节点在 2*i + 2 的位置父节点在（i - 1）/2 的位置如果要做升序排序则要构造最大堆，因为根节点会输出在数组的最后。一开始是一个无序的数组，要先构造最大堆，构造最大堆的逻辑就是从 i = （array.length - 1）/ 2 开始，i – ，即从半数开始即可（因为根据完全二叉树的性质，半数之后的都是叶子结点），然后去构造这些子树为最大堆，直到根节点。当构造好最大堆后，这时根节点肯定为最大值，将根节点与数组最后的数交换，即将最大值输出到最后，这时最大堆被破坏了，需要重新调整，让其符合最大堆的性质，就是从交换后的位置开始，因为如果发生交换了，就说明比较大的节点“上去了”，原来小的父节点“下来了”，但是原来的父节点下来后，是否满足要求呢，要从这个节点继续做调整（整个过程相当于大的节点不停的“浮上去”，小的节点不停的“沉下去”）。那么完成这一操作后，数组就是按照升序排列。既然是一个二叉树，堆排序为什么不用链表实现？其实也是可以的，不过我认为还有几点考虑：1、链表的结构消耗更多的内存2、数组可以提供索引来快速检索3、链表的优势在插入，但堆的数组在插入后的调整也是O(log n)，也不差参考资料https://www.cnblogs.com/Java3y/p/8639937.html]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>排序</tag>
        <tag>算法</tag>
        <tag>HeapSort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于HashMap的一些理解]]></title>
    <url>%2Fposts%2Fdf45eaf1%2F</url>
    <content type="text"><![CDATA[概要本文主要补充对HashMap的一些理解、分析。相信大家对HashMap都很熟悉，但是其中的一些细节上的设计、思想，往往会被大家忽略，这些都是构成HashMap的重要组成部分，包括有“如何做hash”，“resize后如何保证key的位置”，“resize在高并发下引发的死循环”，“为什么 TREEIFY_THRESHOLD = 8？”，“允许null值的原因”等等，希望有你感兴趣的。补充对HashMap的几点理解为什么JDK 1.8后链表改为红黑树当 HashMap 中有大量的元素都存放到同一个桶中时，这个桶下有一条长长的链表，这个时候 HashMap 就相当于一个单链表，假如单链表有 n 个元素，遍历的时间复杂度就是 O(n)，如果 hash 冲突严重，由这里产生的性能问题尤为突显。JDK 1.8 中引入了红黑树，当链表长度 &gt;= TREEIFY_THRESHOLD（8） &amp; tab.length &gt;= MIN_TREEIFY_CAPACITY（64）时，链表就会转化为红黑树，它的查找时间复杂度为 O(logn)，以此来优化这个问题。如何做hash这是JDK1.8优化之后的样子，key.hashCode() 是个 int 即 32位；h &gt;&gt;&gt; 16 表示无符号右移 16 位，即保留高16位；（&gt;&gt;&gt; 意思是右移时无论是正数还是负数，高位统一补0；&gt;&gt; 遇到负数时高位是补1的）然后，用高16位异或低16位，得到新的低16位，得到的结果就是高16位是原来的高16位，低16位是原来高16位和原来低16位的异或结果。为什么要这样做？我们再看看取出数组下标的方法再说。定位到 table[] 的下标就是 (length - 1 ) &amp; hash（原来这一行代码在JDK1.7是一个叫做 indexFor 的方法，JDK1.8把这个方法删掉了）。没错就是通过 &amp; 的操作，通过 &amp; 运算可以获得一个小于 length - 1 的值。size 是保证等于 2 的 N 次方，所以 hash &amp; (size -1 ) 就相当于做取模运算。那么我们回答一下刚刚的问题：既然取模会忽略高位，那么在 size 比较小的情况下，取模结果就取决于低位，譬如 241（11110001） 和 1009（1111110001） 这两个 hashcode 对 size 为16（1111） 的取模结果都是 1，但是这两个数还是相差比较大的嘛，我们的本意是希望尽量的分散。那么 (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16) 的做法就是把高16位加入到低16位去，以此来让低位16位保留高16位的“特征”（高16位是这个 hashcode 的主要特征，这样做法就是可以让低16位也可以表现出这个数的主要特征），同时也加大低16位的随机性。这样做的目的主要是为了提高运算的速度和 hash 的效率，防止 hash 冲突。JDK1.7的hash算法由于“不怎么随机”，发生过类似 DOS 的攻击HASH COLLISION DOS 问题putVal的思路大概思路：对key的hashCode()做hash，然后再计算index；如果没碰撞直接放到bucket里；如果碰撞了，以链表的形式存在buckets后；如果碰撞导致链表过长(大于等于TREEIFY_THRESHOLD)，就把链表转换成红黑树；如果节点已经存在就替换old value(保证key的唯一性)如果bucket满了(超过load factor * current capacity)，就要resize。关于threshold和loadFactor大家都知道 threshold 的作用是当 size 大于 threshold 时就会进行 resize，但是 threshold 的值是多少呢？threshold = capacity * load factorloadFactor 默认为 0.75 是时间和空间上折中考虑。如果太大，虽然会减少空间的占用，但是会增加查询的时间度，因为发生碰撞的几率会提高，从而从 O(1) 退化为链表或者红黑树的查询。resize后如何保证key的位置JDK1.8由于 hash 方法的优化，所以 resize 也受到影响。官方的注释说，经过 rehash 之后，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。为什么会这样？我盗一下图元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1bit(红色)，因此新的index就会发生这样的变化：上面的图说的很明白，如果原来的 hashcode 在高1位有值，那么在“取模”的运算中，这个“1”会被保留下来，所以 new index = old index + oldCap，如果高1位是0，结果还是跟原来一样。这个设计巧妙在于，整体容量扩容了1倍的意义是对每一个 table[i] 都公平的扩容了1倍，而对每个元素是否需要挪到新的 table[i + oldCap]就随机性般的取决于“高1位”是0还是1，在平均的情况下各占50%。我又盗一个图这个图说的很明白，原来在 15 的位置，在 resize 后，蓝色的还是在 15 的位置，绿色就变成在 31 的位置了（31 = 15 + 16）。还有一点注意就是，JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是在JDK1.8不会倒置。resize在高并发下引发的死循环这是在JDK1.7之前才会出现的问题，简单来说就是在高并发下，在内部操作时导致链表死循环引用。参考老生常谈，HashMap的死循环这根本原因是在 rehash 时，链表以倒序进行重新排列。但是在JDK1.8后，这个问题得到了优化这里的代码需要对应到上面有蓝色和绿色两个链表的图。loHead 和 loTail 代表蓝色的那个链表，也即“高1位”不为1的 hashcode 的那些节点，它们 resize 后还是放在原来的位置上。hiHead 和 hiTail 代表绿色的那个链表，也即“高1位”位1的 hashcode 的那些节点，它们 resize 后会放在 oldIndex + oldCap 的位置上。这里可以看出链表是以原来的顺序排列的，tail 节点不停往后追加，head 没有改变，遍历完之后就让 tab[i] 指向 head 就好了。JDK1.8 之后不仅解决了死循环的问题（虽然在并发下还有其他问题），而且代码也更加简洁易懂。为什么TREEIFY_THRESHOLD=8？我们看看官方的注释TREEIFY_THRESHOLD 的作用是链表转为红黑树的阈值，这个之前已经说了。那么为什么是8呢？继续看官方的注释大概意思是如果 hash 很理想，分布就会很平均，tree bins 就会很少用到。在理想的情况下，节点的分布符合柏松分布（Poisson distribution）。我们来分析一下，先看看柏松分布的概率函数我们假设事件 X=k 为某一个 bucket 有 k 个节点。柏松分布只有一个参数就是 λ，那么 λ 为多少呢？官方的说法是Ideally, the frequency of nodes in bins follows a Poisson distribution (http://en.wikipedia.org/wiki/Poisson_distribution) with a parameter of about 0.5 on average, given the resizing threshold of 0.75它说 λ = 0.5，但是我想了大半天都没想明白为什么是 0.5（如果有人知道的话，恳请您告诉我），我觉得有可能它是统计出来的。我说一下我的想法：二项分布的极限是柏松分布，我们可以根据二项分布的期望 λ=np 求出 λ（n 是实验的次数，p 是单次的概率）。如果 n 比较大，p 比较小，所以我们才说满足泊松分布的条件。我们知道如果 hash 很理想，那么分散在每个 bucket 的概率看作一样，p = 1 / s，s 为 bucket 的长度，如果进行了 n 次实验，那么 s = n / 0.75，所以代进去得出 λ = 0.75于是我们可以根据柏松分布得出事件 X=0，X=1 … 的概率分布0: 0.47241: 0.35432: 0.13293: 0.03324: 0.00625: 0.00096: 0.0001可以看出得到的结果跟官方的差不多，X=8 之前的概率累积接近1。也就是说在某一个 bucket 存在多于 8 个节点的概率极低，这就是 TREEIFY_THRESHOLD = 8 的原因。允许 null 值的原因ConcurrentHashmap 和 Hashtable 都是不允许 null 的 key 和 value 的，而 HashMap 允许，这是为什么呢？这样一对比，就很容易联想到是由于并发问题引起的。Doug Lea 是这么说的：The main reason that nulls aren’t allowed in ConcurrentMaps(ConcurrentHashMaps, ConcurrentSkipListMaps) is thatambiguities that may be just barely tolerable in non-concurrentmaps can’t be accommodated. The main one is that ifmap.get(key) returns null, you can’t detect whether thekey explicitly maps to null vs the key isn’t mapped.In a non-concurrent map, you can check this via map.contains(key),but in a concurrent one, the map might have changed between calls.大概意思是，在并发下，如果 map.get(key) = null，ConcurrentMap 无法判断 key 的 value 为null，还是 key 不存在。但是 HashMap 只考虑在非并发下运行，可以用 map.contains(key) 来做判断。大师还说I personally think that allowingnulls in Maps (also Sets) is an open invitation for programsto contain errors that remain undetected untilthey break at just the wrong time. (Whether to allow nulls evenin non-concurrent Maps/Sets is one of the few design issues surroundingCollections that Josh Bloch and I have long disagreed about.)Collections that Josh Bloch and I have long disagreed about.)Doug Lea 大师也说了，自己对 HashMap 允许 null 也是有争议的。这样做只能等到程序报错才发现错误。参考资料https://tech.meituan.com/java_hashmap.htmlhttps://www.jianshu.com/p/281137bdc223]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>hashmap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ScheduledThreadPoolExecutor原理]]></title>
    <url>%2Fposts%2F68efda44%2F</url>
    <content type="text"><![CDATA[概要ScheduledThreadPoolExecutor 是实现任务调度好工具，它的特点是提供了线程池。ScheduledThreadPoolExecutor原理相关类继承关系首先我们看看 ScheduledThreadPoolExecutor 是什么可以看出它是一个 ThreadPoolExecutor，还继承了 ScheduledExecutorService，这个接口定义了诸如 schedule，scheduleAtFixedRate，scheduleWithFixedDelay 等方法。ScheduledThreadPoolExecutor的创建先看看构造函数直接调用 super 的构造方法，即 ThreadPoolExecutor 的，只不过 maximumPoolSize 写死了是 Integer.MAX_VALUE，keepAliveTime 是 0，workQueue 是 DelayedWorkQueue（这个是 ScheduledThreadPoolExecutor 的专用内部 queue，等下会讲到）。为什么 maximumPoolSize = Integer.MAX_VALUE ？我看到网上有的说法是，如果对线程数做了限制，就会对定时任务的调度产生延时（假设任务太多，线程忙不过来），这种说法听上去挺合理的，但是是不正确的。可能 ScheduledThreadPoolExecutor 从一开始设计就没有说要严格准时的执行定时任务，所以压根儿就没有考虑这个问题。通过源码发现，maximumPoolSize 是根本没起作用的，线程的数量不会大于 corePoolSize。为什么 maximumPoolSize 没用？是因为 ScheduledThreadPoolExecutor 的 queue 是无界的（每次达到上限会增长50%，跟 DelayQueue 也即 PriorityQueue 一样；如果对这个答案不明白，你可能需要看看 ThreadPoolExecutor ）。为什么要用无界 queue ？我猜想是 queue 里面的 task 是延迟或周期性的，会长期驻留，对队列的长度有要求，如果公开给调用者设置或者给一个固定的值，都不合适，会产生问题，所以干脆无界。还有另外一个原因，设置了 maximumPoolSize 且有效，如果此时 wc &gt; corePoolSize，且队列头的任务 delay 很大，那么在高并发的情况下，会不断有 worker 新建和销毁，造成性能问题，甚至 GC。为什么 keepAliveTime = 0 ？一般情况下 maximumPoolSize 不起作用，那么 keepAliveTime 也是不起作用的。但是也可以通过 allowCoreThreadTimeOut 令到 keepAliveTime 生效（通过调用 allowCoreThreadTimeOut(true) 方法设置），但是这个 keepAliveTime 确实不好设置，试想如果 keepAliveTime 小于队列头的 delay，那么这个线程就会被回收掉，然后在下次又创建一个新的线程，这不是很多余吗，所以干脆 keepAliveTime = 0。定时任务的执行schedule方法ScheduledThreadPoolExecutor 覆盖了 AbstractExecutorService 的 submit 方法，submit 也是直接调用 schedule 方法，我们一般使用也是调用 schedule，我们看看 schedulescheduleWithFixedDelay 和 scheduleAtFixedRate 都是类似的，只有一个参数的区别，所以我一起讲scheduleWithFixedDelay 和 scheduleAtFixedRate 两个方法的区别，相信大家都知道，前者是上一次任务执行完，再延迟 delay 的时间再执行下一次，后者是上一次任务的执行开始时间加上 period 就是下一次任务的执行时间。我们看到 scheduleWithFixedDelay 和 scheduleAtFixedRate 基本是一样的，就只有当传到 ScheduledFutureTask 的入参时，delay 变成了一个负数，period 还是一样，这一点大家先记住，后面会用到。继续讲 schedule 方法decorateTask 方法只是让 ScheduledFutureTask 变成 RunnableScheduledFuture，使得 delayedExecute 更加通用ScheduledFutureTask 是 ScheduledThreadPoolExecutor 内部定义的任务类，从结构看，简单来说它就是一个 FutureTask + Delayed我们看看 ScheduledFutureTask 构造方法如果是 schedule 则 period 为0，scheduleWithFixedDelay 和 scheduleAtFixedRate 则等于入参，这就是一次性任务和周期性任务的区别如果是 schedule 则 period 为0，scheduleWithFixedDelay 和 scheduleAtFixedRate 则等于入参，这就是一次性任务和周期性任务的区别继续看看 delayedExecuteshutdown 就直接 reject；否则加入到队列，再发现是 shutdown 的话，就 remove 掉，中断 task；这里为什么直接加入队列？因为任务的延迟的，一定要确保从延迟队列中取出来运行。最后调用 ensurePrestart 确保有 worker 在运行；这里回应上面的， wc &lt; corePoolSize，所以 maximumPoolSize 是没用的。把任务加到队列了，注意由于队列 DelayedWorkQueue 是类似 DealyQueue，这涉及到 task 的 getDelay 和 compareTo （还记得上面说 ScheduledFutureTask 是一个 Delayed 吗 ），还有这个 queue 是一个二叉堆，涉及 siftUp 和 siftDown 的堆操作，这部分都跟 DealyQueue 比较相关，这里就不展开了。接下来就是 worker 从队列取出任务，取法跟 ThreadPoolExecutor 一样。run方法接下来就是 task 的运行ScheduledFutureTask 是一个 FutureTask，它覆盖了 run 方法canRunInCurrentRunState，刚刚我们在 delayedExecute 也遇到，它使用来判断线程池是否在运行 RUNNING，如果是 SHUTDOWN，是否允许终止任务；continueExistingPeriodicTasksAfterShutdown 意思是，对于周期性任务，在 SHUTDOWN 下，是否允许继续执行，默认是 false；executeExistingDelayedTasksAfterShutdown 意思是，对于非周期性任务，在 SHUTDOWN 下，是否允许继续执行，默认是 true；我们回到重点来，看红箭头。如果是非周期性任务，那么就调用 FutureTask 的 run 方法；如果是周期性任务，那么就调用 FutureTask 的 runAndReset 方法（runAndReset 跟 FutureTask 相关，这里不展开了），简单说就是这个 future 执行完之后会重置为 NEW 状态；setNextRunTime方法setNextRunTime 方法，计算任务下一次的执行时间（还记得上面我们说 delay 是负数，period 是原值吗？这里用到了，这两个值都是对应到这里的 period）如果 p &gt; 0 ，则在原来的时间上 time 直接追加 period，否则在 now() 的基础上追加triggerTime 获取下次执行任务的时间triggerTime防溢出这里还有一个巧妙的地方，我得说一下为什么 delay 要跟 Long.MAX_VALUE 右移一位比较？不急，我们先看看 overflowFree 方法注释已经把大意说清楚了，就是为了防止溢出。因为 head 的 getDealy 有可能是负数（一直没有出队运行），那么当前 task 加入队列时做 compareTo 就有可能溢出（减去一个负数得到一个大于 Long.MAX_VALUE 的数），那么这时比较的结果就不对了。首先 delay 肯定不为负数，我们分情况看一下：1、如果 headDealy 为正数（含0），两个正数相减不会溢出，这没问题2、如果 headDealy 为负数，那么只要 delay - headDealy &gt; Long.MAX_VALUE 就不是我们想要的结果，所以要对 delay 或 headDealy 做一下限制。我们回到刚刚提出的问题（ delay &lt; (Long.MAX_VALUE &gt;&gt; 1) ？）。之所以有这个做法，是因为对 delay 和 headDealy 的值做了一个折中。如果 delay &lt; (Long.MAX_VALUE &gt;&gt; 1) （Long.MAX_VALUE &gt;&gt; 1 就是 Long.MAX_VALUE 的一半），那么就直接用这个 delay 进队；如果大于的话，那就认为它做 compareTo 时极有可能会溢出（这个是人为的认为），那么就取出 headDealy 来试一下，真溢出了，就做调整。这里巧妙的地方在于，它给了 delay 和 headDealy 的值 Long.MAX_VALUE 的一半这么多的预留空间（各占一半），试想如果把 delay &lt; (Long.MAX_VALUE &gt;&gt; 1) 改为 delay &lt; Long.MAX_VALUE（极端为 delay = Long.MAX_VALUE 的情况），那么 headDealy 只要小于 0 就会溢出。所以只要 headDealy 大于 Long.MIN_VALUE &gt;&gt; 1 就不会溢出。当然，headDealy 是有可能小于 Long.MIN_VALUE &gt;&gt; 1 的，所以为了万一，最后还是会做调整。reExecutePeriodic方法我们继续回到重点 reExecutePeriodic 方法跟之前讲解的代码有点像，相信大家都看的明白了，主要就是把 task 加回到 queue 里。关闭线程池ScheduledThreadPoolExecutor 的 shutdown 和 shutdownNow 都是直接调用 ThreadPoolExecutor 的。至此，ScheduledThreadPoolExecutor 的大概流程和原理讲得7788了。Why DelayedWorkQueue?这里补充一下我在看 ScheduledThreadPoolExecutor 源码时心里最大的一个疑问。为什么不直接用 DealyQueue ，而是另外写了一个 DelayedWorkQueue？不过还好不用我们自己瞎猜，官方的注释给出了说明简单来说就是 DelayedWorkQueue 其实跟 DealyQueue 差不多，不过里面的元素 ScheduledFutureTask 会记录在堆的下标，做 remove 的时候时间复杂度从 O(n) 提升到 O(log n)。 所以 DelayedWorkQueue 重写了remove 方法，直接取出元素的 index。原来 DealyQueue 的做法是遍历数组找出元素的下标（如果元素不是 ScheduledFutureTask 类型也是这样做）+ 堆操作：O(n) + O(log n) 约等于 O(n) .DelayedWorkQueue 的操作变为直接取出下标 + 堆操作：O(1) + O(log n) 约等于 O(log n)总的时间复杂度从 O(n) -&gt; O(log n)总结ScheduledThreadPoolExecutor的实现跟 ThreadPoolExecutor类似，它利用了延迟队列 DealyQueue 对任务进行延迟运行。参考资料https://www.jianshu.com/p/2756fd08d0cdhttps://www.jianshu.com/p/d96e9f67dba5Java多线程复习与巩固（七）–任务调度线程池ScheduledThreadPoolExecutor]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ScheduledThreadPoolExecutor</tag>
        <tag>调度线程池</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程池ThreadPoolExecutor实现原理]]></title>
    <url>%2Fposts%2Ff5cda8d1%2F</url>
    <content type="text"><![CDATA[概要线程池，大家都很熟悉了，我们在平时应用中也用的很多。对线程池，ThreadPoolExecutor 的实现原理有一定理解后，我们可以把它用的更好，对它的参数有更加深刻的理解，甚至我们可以扩展，监控自己的线程池。ThreadPoolExecutor实现原理本文代码基于JDK1.8线程池相关的类的关系我们先看看主要的类ThreadPoolExecutor的继承关系平时可能还有用到的 Executors 类，这是一个工具类，提供newFixedThreadPoolnewCachedThreadPoolnewScheduledThreadPool等静态方法方便我们创建线程池，最终还是调用 ThreadPoolExecutor 来创建的，一般规范不建议直接使用 Executors 来创建线程池。线程池创建的主流程线程池的状态先看看线程池的状态有哪些，对它有初步的理解线程池的状态和运行的线程数只用了一个 int，其中高3位表示状态，低29位表示线程数。状态表示的意思和状态之间的转换：RUNNING - 可以接受新的任务，及执行队列中的任务SHUTDOWN - 不接受新的任务，但会继续执行队列中的任务STOP - 不接受新的任务，既不会执行队列中的任务，还会中断执行中的任务TIDYING - 全部任务已经终止，且线程数为0TERMINATED - 线程池完全终止RUNNING -&gt; SHUTDOWN - 执行了 shutdown()(RUNNING or SHUTDOWN) -&gt; STOP - 执行了shutdownNow()SHUTDOWN -&gt; TIDYING - 队列和线程为空STOP -&gt; TIDYING - 线程为空TIDYING -&gt; TERMINATED - terminated() 这个勾子方法执行完毕线程池的创建ThreadPoolExecutor 的构造函数线程池的创建只是初始化了一些参数，但理解好这些参数对我们使用线程池很有帮助。corePoolSize - 核心线程数，除非 allowCoreThreadTimeOut 为 true（默认false），否则即使没有任务，也会维持这么多线程。maximumPoolSize - 最大线程数，corePoolSize 满了的话，会把任务放到队列，如果队列满了的话（假设队列有界），就会继续创建线程直到 maximumPoolSize，如果超过 maximumPoolSize 则会执行 reject 策略。workQueue - 用来存放任务的队列，是一个 BlockingQueue，常用的有 LinkedBlockingQueue，ArrayBlockingQueue，SynchronousQueue。keepAliveTime - 空闲线程的存活时间，如果当前线程数 &gt; corePoolSize，且某个线程空闲了这么多时间（没有获取到任务并运行），那么这个线程会被 remove 掉。unit - keepAliveTime 的单位，内部会统一转换成 nanosthreadFactory - 用来创建线程的 ThreadFactory，主要用来给线程命名（方便查看日志），设置 daemon，优先级和 group 等，Executors 有 DefaultThreadFactory 这个默认实现。handler - reject 具体执行策略，reject 的条件上面已经说了，一般内置有以下几个，也可以自己实现CallerRunsPolicy - 不使用线程池的线程执行该任务，直接用当前执行任务（例如 main 线程）AbortPolicy - 直接抛异常DiscardPolicy - 无视不做处理，相当于抛弃掉DiscardOldestPolicy - 将队列头的任务取出来抛弃掉，然后运行当前任务线程池的执行一般我们使用 ExecutorService 的 submit 方法来使用线程池执行一个任务，这个方法调用到 AbstractExecutorService这里我们看到所有 task 无论是 Callable 还是 Runnable 的都会包装成一个 RunnableFuture 也就是 FutureTask，返回给我们。execute方法重点看 execute 方法，调用了 ThreadPoolExecutor 的 execute我们分三种情形来看，每个是一个 if 条件：第一，当 workCount &lt; corePoolSize 时，直接创建 worker 线程；第二，如果上面创建失败（可能是线程池正在处于关闭状态，可能是 workCount &gt; corePoolSize 了 - 并发场景），那么这时把任务放入 workQueue 队列；下面的判断是用来防止，线程池不在运行了，就把任务删掉；如果没有线程了就加一个；第三，来到这步说明上面放队列不成功（可能是队列是有界的，满了），那么就继续创建线程来满足，如果这都创建失败（可能是 &gt; maximumPoolSize）就 reject 了；addWorker方法继续看看重点的 addWorker 方法，addWorker 分开两部分来看。这一步是判断是否可以增加 worker 的重点：第一，首先开始的判断有点奇怪，我也不是很明白，先认为它是如果状态是 SHUTDOWN 则不允许创建线程；第二，下面有个 core 参数，true 使用 corePoolSize，false 使用 maximumPoolSize，我们上面说的 execute 方法第一次就是传 true 的，第二次就传 false。所以这里就是对 size 做判断， 如果 &gt;= size 则返回 false，否则 cas 一下，成功了就 break 执行下面的代码；第三，如果 cas 失败，说明有其他并发竞争，则 cintinue 循环上面的步骤判断。来到这一步说明可以创建 worker 了，这里用了一个全局 lock，来控制创建线程和关闭线程的不能同时做。可以看到步骤就是 new 一个 worker，add 到 workers 里，workers 是一个 HashSet。largestPoolSize 来用记录最大的线程数。如果 workerStarted == false（线程池在关闭或创建 worker 时异常），则会 addWorkerFailed 方法。主要就是 remove 掉 worker，扣减计数，这里还会调用 tryTerminate 。这个方法会在很多地方用到，它的作用就是防止线程池在终止状态这种情形，去终止线程。Worker是什么我们刚刚一直说 worker，那到底 Worker 究竟是什么？我们现在来看看我们可以看到 Worker 是一个 AQS 和 Runnable。为什么是一个 AQS ？我们可以结合注释和代码可以得到，worker 在跑任务的时候会 lock 住，在被中断时会 tryLock，利用上锁这一点来区分这个 worker 是否空闲。Worker 中重写 AQS 的方法。（感概：AQS 真是个简单易用，用于并发控制的好工具！）为什么是一个 Runnable ？我们看看 Worker 的构造函数，在创建 Thread 时把自己 this 传到 thread 的参数，说明 worker 的 thread 跑的是自己，这时我们就知道 worker 的入口了。Worker 的 run 方法runWorker方法重点的 runWorker 方法task 可能是传进来的 firstTask 或者 getTask() 获取到的，getTask 也是重点方法，等下讲到；运行 task 时会上锁，锁的作用我刚刚已经说了；如果线程池状态是 STOP 则中断线程；这里放了两个勾子 beforeExecute 和 afterExecute 方法来提供给子类做点事情，一般用于监控或统计线程池的执行情况；执行任务就直接 task.run() 了，还记得我说过这个 task 是一个 FutureTask，如果run 的时候抛出异常，FutureTask 会 catch 掉不会再 throw（如果大家对 FutureTask 不熟悉就先这样理解），所以这里不会进入 catch，也就是不会 throw x 了。如果 task 不像 FutureTask 一样自己处理掉异常，那就会 throw x 了，那么 worker 的线程就会跳出 while 循环，完成使命，结束自己；获取不到 task （task 为null）或者循环过程中异常，最后都会执行 processWorkerExit。processWorkerExit 的作用主要就是 remove 掉 worker，那么扣减 workCount 呢？好像没有看到。这里用了 completedAbruptly 这一变量来控制是否在 processWorkerExit 扣减 workCount，因为有可能是在 getTask 已经扣减了，那么在 processWorkerExit 就不用重复扣减。我们结合 runWorker 来看看，分两种情况：1、如果 firstTask 为 null，那么会走到 getTask 方法，如果 getTask 返回 null，那么说明已经是扣减了，这时退出循环，completedAbruptly = false，不用重复扣减。2、如果 firstTask 不为 null（1）执行 firstTask 正常结束，然后循环，走到 getTask，如果返回 task 为 null，那么 completedAbruptly = false，不用重复扣减。（2）执行 firstTask 执行异常，这时 completedAbruptly = true，需要扣减这里我们又看到 tryTerminate 了；下面的判断主要是尝试去增加一个 worker，因为你 remove 掉了一个，如果条件允许，那就加回一个呗。getTask方法看看重点的 getTask 方法在 getTask 时如果发现时线程池在关闭状态，那么就需要停止获取任务了；如果 wc &gt; maximumPoolSize，超过了最大 size 了，就去 cas 扣减 workCount 一下，成功就返回 null；如果 wc &gt; corePoolSize（小于 maximumPoolSize），且 timedOut 的话，说明这个 worker 也有点“多余”，也去扣减 workCount。注意这里对 timedOut 的判断，通过 queue 的定时 poll 方法，时间是线程池的构造参数 keepAliveTime，如果过了这么久都还没获取 task，说明 queue 是空的，有空闲的线程，那就把这个 worker remove 掉吧；如果 wc &lt; corePoolSize 的话，那就调用 queue 的 take 方法一直阻塞获取 task；还有 allowCoreThreadTimeOut 参数，它的意思是忽略对 corePoolSize 的判断。关闭线程池上面已经把线程的创建和执行基本说得7788了，我们看看关闭线程池是如何做的，其实在上面的很多方法中，都看到很多对如 SHUTDOWN 的判断了。主要有 shutdown 和 shutdownNow 这两个方法。这两个方法很相似，从名字来看 shutdown 显得更加的柔性，实际也是。shutdown –不接受新的 task，在运行和已经在队列的 task 可以继续运行；把状态改为 SHUTDOWN；中断空闲 worker，这个在上面已经提到过了，用锁对是否空闲做判断。interruptIdleWorkers 打断空闲的线程这里还有个 onShutdown 勾子方法。shutdownNow –不接受新的 task，中断已经在运行的线程，清空队列；把状态改为 STOP；强制中断所有在运行 worker 的线程；drainQueue，相当于把队列的 task 丢弃掉；总结线程池ThreadPoolExecutor实现的原理，就是用 Worker 线程不停得取出队列中的任务来执行，根据参数对任务队列和 Workers 做限制，回收，调整。参考资料http://www.jianshu.com/p/87bff5cc8d8chttps://javadoop.com/post/java-thread-pool]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>源码</tag>
        <tag>线程池</tag>
        <tag>ThreadPoolExecutor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正则回溯分析]]></title>
    <url>%2Fposts%2F2fa47d3c%2F</url>
    <content type="text"><![CDATA[概要这是对一次线上正则回溯引起的问题的分析，文章对当时问题进行了简化，从而深入源码分析正则引擎是如何进行回溯以及回溯的时间复杂度。正则回溯分析排查过程zabbix接受到警告之后在 zabbix 看到 cpu 直飚 100%，当堂一惊！（我是CPU，现在慌得一比）。看到是 5.27 后开始飚升，首先怀疑代码问题，认真翻了一下 5.27 前 commit 到 master 的代码，没有发现明显的死循环或者死锁。于是叫运维帮忙看一下机器状态和 dump 出 jstack。top &amp; jstack线上机器 top：dump：定位问题可以看出跟正则有关的调用栈很长，于是把问题定位在 validateUrl 方法上。这是一个用正则去校验一个外部电子发票链接url的方法，其中 url=1http://www.fapiao.com/dzfp-web/pdf/download?request=6e7JGmpM5neXWMVrv4ILd-kEn64HcUX4qL4a4qJ4-CEk7Azg.Vjit92m74H5oxkjgdsYazxcUmdJjKscGXhaJw__%5EHGabjgEIe其中，正则 pattern =1^([hH][tT]&#123;2&#125;[pP]://|[hH][tT]&#123;2&#125;[pP][sS]://)(([A-Za-z0-9-~]+).)+([A-Za-z0-9-~\/])+$原因分析阅读前提阅读下文时，我希望你对正则的基本用法、基本概念，正则的贪婪、懒惰、独占都有一定的理解。之前，我的一个同事已经对这个做了一定的分析，大家可以先阅读一下 藏在正则表达式里的陷阱回溯的定义在网上看了一下大致都说这个是跟正则的回溯有关，那究竟什么是正则的回溯呢？下面是我在网上找到一个比较好解释，可能看了之后还是懵逼，不怕，接下来会有详细解释使用 NFA 引擎的模式匹配由正则表达式中的语言元素驱动，而不是由输入字符串中要匹配的字符驱动。 因此，正则表达式引擎将尝试完全匹配可选或可替换的子表达式。 当它前进到子表达式中的下一个语言元素并且匹配不成功时，正则表达式引擎可放弃其成功匹配的一部分，并返回以前保存的与将正则表达式作为一个整体与输入字符串匹配有关的状态。 返回到以前保存状态以查找匹配的这一过程称为回溯。简化还原分析由于原来的url和pattern太长有点复杂，不好做分析，所以我进行了简化，方便做调试和分析。根据正则的贪婪和回溯特性，我做了如下的简化。（如果你对正则有一定的理解，相信你也会对原来url和pattern做到如下的简化）url=1aaaaaaaaaaaa_ （只是想表示有 n 个 a）pattern=1(a+)+以下的分析都是基于上面的ur和pattern。（基于JDK8）通过调试代码，我发现匹配字符的类最后是在 CharProperty 的 match 方法；其中通过 Character.codePointAt(seq, i) 这个方法获取需要匹配的字符，其实这个方法终于还是调用 CharSequence 的 charAt 这个方法123456789101112131415161718192021222324252627/** * Abstract node class to match one character satisfying some * boolean property. */private static abstract class CharProperty extends Node &#123; abstract boolean isSatisfiedBy(int ch); CharProperty complement() &#123; return new CharProperty() &#123; boolean isSatisfiedBy(int ch) &#123; return ! CharProperty.this.isSatisfiedBy(ch);&#125;&#125;; &#125; boolean match(Matcher matcher, int i, CharSequence seq) &#123; if (i &lt; matcher.to) &#123; int ch = Character.codePointAt(seq, i); return isSatisfiedBy(ch) &amp;&amp; next.match(matcher, i+Character.charCount(ch), seq); &#125; else &#123; matcher.hitEnd = true; return false; &#125; &#125; boolean study(TreeInfo info) &#123; info.minLength++; info.maxLength++; return next.study(info); &#125;&#125;于是我在 stackoverflow 找到一个继承 CharSequence 的类来做一些辅助 InterruptableCharSequence ，主要是打印当前匹配的字符和匹配次数，还有后面要做的中断。12345678910111213141516171819202122232425262728293031323334353637383940414243public class InterruptableCharSequence implements CharSequence&#123; CharSequence inner; public long counter = 0; public InterruptableCharSequence(CharSequence inner) &#123; super(); this.inner = inner; &#125; public long getCounter()&#123; return counter; &#125; @Override public char charAt(int index) &#123; boolean isInterrupt = Thread.currentThread().isInterrupted(); if(isInterrupt)&#123; System.out.println("currentThread has been set interrupt"); &#125; if (Thread.interrupted()) &#123; // clears flag if set System.out.println("interrupt !!!"); throw new RuntimeException(new InterruptedException("occur from InterruptableCharSequence")); &#125; counter++; System.out.println("charAt = " + inner.charAt(index)); return inner.charAt(index); &#125; @Override public int length() &#123; return inner.length(); &#125; @Override public CharSequence subSequence(int start, int end) &#123; return new InterruptableCharSequence(inner.subSequence(start, end)); &#125; @Override public String toString() &#123; return inner.toString(); &#125;&#125;推出时间复杂度通过调试代码和根据打印信息，可以得出正则回溯的匹配过程：假设 url=a_，pattern=(a+)+(1) a 匹配，继续(2) _ 不匹配以上两步是第一个 + 的匹配过程(3) 尝试匹配 _ 看看是不是可以结束以上这一步是第二个 + 的匹配过程(4) 没有回溯，结束（没有回溯是没有发生贪婪，发生贪婪的条件是从第一个字符匹配成功后，下一个字符又匹配成功）所以这时一共匹配了 3 步，匹配顺序为：1a _ _假设 url=aa_，pattern=(a+)+(1) ~ (3) 匹配到了 aa_(4) 尝试匹配 _ 看看是不是可以结束(5) 发生回溯，后退一步，递归 a_ 的匹配过程(n) 最终还是匹配不成功，结束所以这时一共匹配了 7 步，匹配顺序为：1a a _ _ a _ _假设 url=aaa_，pattern=(a+)+(1) ~ (4) 匹配到了 aaa_(5) 尝试匹配 _ 看看是不是可以结束(6) 发生回溯，后退一步，递归 a_ 的匹配过程(…) 上一步最终还是匹配不成功的，于是又后退一步，递归 aa_ 的匹配过程(n-1) 直到回退到第一个 a，回溯结束，已经遍历了所有的情况(n) 最终还是匹配不成功，结束所以一共匹配了 15 步，匹配顺序为：1a a a _ _ a _ _ a a _ _ a _ _根据上面我们可以推断出时间复杂度：1234567f(1) = 1f(2) = 3 = 2 + f(1)f(3) = 7 = 3 + f(2) + f(1)f(4) = 15 = 4 + f(3) + f(2) + f(1)f(n) = n + f(n-1) + f(n-2) + ... + f(1)所以 f(n) = 2 的N 次方 - 1可见恐怖！！回溯源码分析现在我们来回头看看正则回溯的相关代码，主要是在 Curly 的 match0 方法1234567891011121314151617181920212223242526272829303132333435363738394041// Greedy match.// i is the index to start matching at// j is the number of atoms that have matchedboolean match0(Matcher matcher, int i, int j, CharSequence seq) &#123; if (j &gt;= cmax) &#123; // We have matched the maximum... continue with the rest of // the regular expression return next.match(matcher, i, seq); &#125; int backLimit = j; while (atom.match(matcher, i, seq)) &#123; // k is the length of this match int k = matcher.last - i; if (k == 0) // Zero length match break; // Move up index and number matched i = matcher.last; j++; // We are greedy so match as many as we can while (j &lt; cmax) &#123; if (!atom.match(matcher, i, seq)) break; if (i + k != matcher.last) &#123; if (match0(matcher, matcher.last, j+1, seq)) return true; break; &#125; i += k; j++; &#125; // Handle backing off if match fails while (j &gt;= backLimit) &#123; if (next.match(matcher, i, seq)) return true; i -= k; j--; &#125; return false; &#125; return next.match(matcher, i, seq);&#125;从代码和注释得知，开始匹配成功后，就会进入贪婪模式，直到匹配不成功，然后开始发生回溯，用 backLimit 这个变量记录最开始匹配成功的下标，即允许回溯最后的地方。开始发生回溯的地方，即第33行的 next.match 方法，这个 next 指的是下一个节点（因为 java 实现 NFA 用了类似图（graph）的数据结构，匹配的地方，group开始和结束的地方等等都抽象成一个个 Node），由于第二个 + 的原因，构成了一个有环的图，于是发生递归。以下是我调试时根据 Node 的关系，画出来的图：线上问题的url分析&amp;解决现在回过头来看看导致线上问题的原因：先把正则简化一下， pattern=1^(http://)(([A-Za-z0-9-~]+).)+$加点打印信息，可以看出从第24行开始发生回溯 ：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171181191201211221231241251261271281291301311321331341351361371381391401411421431441451461471481491501511521531541551561571581591601611621631641651661671681691701711721731741758 ww.fapiao.com/dzfp-web/pdf/download?request=6e7JGmpM5neXWMVrv4ILd-kEn64HcUX4qL4a4qJ4-CEk7Azg.Vjit92m74H5oxkjgdsYazxcUmdJjKscGXhaJw__%5EHGabjgEIe 12 apiao.com/dzfp-web/pdf/download?request=6e7JGmpM5neXWMVrv4ILd-kEn64HcUX4qL4a4qJ4-CEk7Azg.Vjit92m74H5oxkjgdsYazxcUmdJjKscGXhaJw__%5EHGabjgEIe 19 om/dzfp-web/pdf/download?request=6e7JGmpM5neXWMVrv4ILd-kEn64HcUX4qL4a4qJ4-CEk7Azg.Vjit92m74H5oxkjgdsYazxcUmdJjKscGXhaJw__%5EHGabjgEIe 23 zfp-web/pdf/download?request=6e7JGmpM5neXWMVrv4ILd-kEn64HcUX4qL4a4qJ4-CEk7Azg.Vjit92m74H5oxkjgdsYazxcUmdJjKscGXhaJw__%5EHGabjgEIe 32 df/download?request=6e7JGmpM5neXWMVrv4ILd-kEn64HcUX4qL4a4qJ4-CEk7Azg.Vjit92m74H5oxkjgdsYazxcUmdJjKscGXhaJw__%5EHGabjgEIe 36 ownload?request=6e7JGmpM5neXWMVrv4ILd-kEn64HcUX4qL4a4qJ4-CEk7Azg.Vjit92m74H5oxkjgdsYazxcUmdJjKscGXhaJw__%5EHGabjgEIe 45 equest=6e7JGmpM5neXWMVrv4ILd-kEn64HcUX4qL4a4qJ4-CEk7Azg.Vjit92m74H5oxkjgdsYazxcUmdJjKscGXhaJw__%5EHGabjgEIe 53 e7JGmpM5neXWMVrv4ILd-kEn64HcUX4qL4a4qJ4-CEk7Azg.Vjit92m74H5oxkjgdsYazxcUmdJjKscGXhaJw__%5EHGabjgEIe 102 jit92m74H5oxkjgdsYazxcUmdJjKscGXhaJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 136 Jw__%5EHGabjgEIe 138 __%5EHGabjgEIe 135 aJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 134 haJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 136 Jw__%5EHGabjgEIe 138 __%5EHGabjgEIe 133 XhaJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 136 Jw__%5EHGabjgEIe 138 __%5EHGabjgEIe 135 aJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 132 GXhaJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 136 Jw__%5EHGabjgEIe 138 __%5EHGabjgEIe 135 aJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 134 haJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 136 Jw__%5EHGabjgEIe 138 __%5EHGabjgEIe 131 cGXhaJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 136 Jw__%5EHGabjgEIe 138 __%5EHGabjgEIe 135 aJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 134 haJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 136 Jw__%5EHGabjgEIe 138 __%5EHGabjgEIe 133 XhaJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 136 Jw__%5EHGabjgEIe 138 __%5EHGabjgEIe 135 aJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 130 scGXhaJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 136 Jw__%5EHGabjgEIe 138 __%5EHGabjgEIe 135 aJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 134 haJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 136 Jw__%5EHGabjgEIe 138 __%5EHGabjgEIe 133 XhaJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 136 Jw__%5EHGabjgEIe 138 __%5EHGabjgEIe 135 aJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 132 GXhaJw__%5EHGabjgEIe 138 __%5EHGabjgEIe 137 w__%5EHGabjgEIe 136 Jw__%5EHGabjgEIe于是，可选的解决方案有：使用可选限定符或替换构造的回溯123(http://)(([\\S]+).)(http://)(([\\S]+?).) (http://)(([A-Za-z0-9-~_%]+?).)非回溯子表达式1^(http://)(?&gt;([A-Za-z0-9-~]+).)+$如何避免综上，我个人总结以下四点来规避上面的正则回溯问题。充分考虑输入除了考虑正确的输入外，更重要的是考虑不匹配的输入！发生回溯都是因为不匹配导致的，正则会不停的尝试匹配，直到所有可能的情况。推荐一个探测工具：https://regex101.com/控制回溯发生回溯是因为正则用到了量词（quantifier）和 替换（alternation）（因为这两者为正则的匹配提供了可能性）。可以加上使用断言（assertions）或 独占模式（possessive），这样可以减少回溯的次数或者避免回溯，但是加上了断言和独占就要考虑对原来的匹配有没有产生影响，匹配结果是否还是一致。量词：?, *, +, {n，m}替换：[x|y] 类似这种断言：(?=exp), (?!exp) 类似这种独占模式：在量词后面再加上一个 +，表示匹配到此为止，不会回吐字符，即不会回溯。使用超时机制但是本人认为回溯是不能避免的，那么就可以使用超时机制，用中断线程的方法来强制结束线程，不要让它在死跑，耗尽CPU资源以下是本人写的一个小测试：12345678910111213141516171819202122232425262728293031323334public class RegexBug &#123; private static String regex3 = "(a+)+"; private static String harmful_url = "aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa_"; private static Pattern URL_PATTERN; static ExecutorService threadPool = Executors.newCachedThreadPool(); public static void main(String[] args)throws Exception &#123; URL_PATTERN = Pattern.compile(regex3, Pattern.MULTILINE); long l1 = System.nanoTime(); CharSequence cs = new InterruptableCharSequence(harmful_url); Future&lt;Boolean&gt; future = null; future = threadPool.submit(() -&gt; validateUrl(cs)); try&#123; Boolean matchResult = future.get(5, TimeUnit.SECONDS); System.out.println("matchResult = " + matchResult); &#125;catch (TimeoutException e)&#123; e.printStackTrace(); future.cancel(true); &#125;catch (Exception e1)&#123; e1.printStackTrace(); &#125; System.out.println("pattern耗时 = " + (System.nanoTime() - l1) / (1000000)); System.out.println("counter = " + ((InterruptableCharSequence) cs).getCounter()); &#125; public static boolean validateUrl(CharSequence url) &#123; Matcher matcher = URL_PATTERN.matcher(url); return matcher.matches(); &#125; &#125;日志：使用现成工具校验与其自己写正则担心写出bug，不如用现成的工具。apache common-validator 了解一下参考资料https://www.cnblogs.com/study-everyday/p/7426862.htmlhttp://wwaw.cnblogs.com/chanshuyi/archive/2018/06/19/9197164.html7164.html]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>正则</tag>
        <tag>回溯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo高级玩法]]></title>
    <url>%2Fposts%2Fbe8242cc%2F</url>
    <content type="text"><![CDATA[概要添加一些高级功能，可以让我们的网站显得更加丰富，多样性，简单说就是更高逼格。高级功能文章阅读统计我选择了 LeanCloud，这也是官方推荐使用的。网上有很多选择不蒜子的，也是可以的。注册 LeanCloud 账号Leancloud官网创建一个应用名字你喜欢就行创建一个Class点击进去应用注意名字必须为 Counter，勾选无限制的权限。修改主题配置修改 next 主题的_config.yml ，找到 leancloud_visitors ，修改为1234leancloud_visitors: enable: true app_id: xxx app_key: xxx其中 app_id 和 app_key 在 LeanCloud 的设置 -&gt; 应用 Key 可以找到重启查看这样就配置好了，重新生成 hexo 并发布，我们就可以看到文章阅读次数的统计。需要特别说明的是：记录文章访问量的唯一标识符是文章的发布日期以及文章的标题，因此请确保这两个数值组合的唯一性，如果你更改了这两个数值，会造成文章阅读数值的清零重计。在 LeanCloud 的后台我们可以看到一个整体的统计量，其中 time 字段就是统计数字，可以修改的哦。安全因为AppID以及AppKey是暴露在外的，因此如果一些别用用心之人知道了之后用于其它目的是得不偿失的，为了确保只用于我们自己的博客，建议开启Web安全选项，这样就只能通过我们自己的域名才有权访问后台的数据了，可以进一步提升安全性。评论功能在网上找了很多，有多说，畅言，来必力，gticomment，valine，选择 valine 是因为搞阅读统计的时候已经注册了 LeanCloud，可以顺手用上，而且 next 已经支持了 valine，可以简单快速用起来。在 LeanCloud 注册和创建应用上面已经做了修改主题配置文件找到 valine 配置项打开 enable，输入 appid 和 appkey ，其他自己设置。12345678910valine: enable: true appid: xxx appkey: xxx notify: false # mail notifier , https://github.com/xCss/Valine/wiki verify: false # Verification code placeholder: 走过路过，不留下点什么吗？ # comment box placeholder avatar: mm # gravatar style guest_info: nick,mail,link # custom comment header pageSize: 10 # pagination size效果这样就我们已经配置好了，重启hexo，看到文章底部出现评论框测试一下hexo d 发布后测试评论一条然后在 LeanCloud 后台可以看到，可以进行删除等操作。关闭评论如需取消某个 页面/文章 的评论，在 md 文件的 front-matter 中增加1comments: false字数统计使用 hexo-wordcount 插件，因为 next 主题已经支持了在 hexo 目录执行安装1npm i --save hexo-wordcount修改主题配置找到 post_wordcount 项123456post_wordcount: item_text: true wordcount: true # 单篇 字数统计 min2read: true # 单篇 阅读时长 totalcount: false # 网站 字数统计 separated_meta: true修改显示文字字数统计和阅读时长是没有单位，需要补上才比较清晰。修改以下文件1themes/next/layout/_macro/post.swig修改【字数统计】，找到如下代码：123&lt;span title=&quot;&#123;&#123; __(&apos;post.wordcount&apos;) &#125;&#125;&quot;&gt; &#123;&#123; wordcount(post.content) &#125;&#125;&lt;/span&gt;修改后为：123&lt;span title=&quot;&#123;&#123; __(&apos;post.wordcount&apos;) &#125;&#125;&quot;&gt; &#123;&#123; wordcount(post.content) &#125;&#125; 字&lt;/span&gt;同理，我们修改【阅读时长】，修改后如下：123&lt;span title=&quot;&#123;&#123; __(&apos;post.min2read&apos;) &#125;&#125;&quot;&gt; &#123;&#123; min2read(post.content) &#125;&#125; 分钟&lt;/span&gt;修改完成后，重新执行启动服务预览就可以了。如下：这个阅读的速度是可以修改的，默认是中文300，英文160字/每分钟，详细可以看 hexo-wordcount。站内搜索就是可以在你的网站搜索你网站内的内容安装 hexo-generator-searchdb在站点的根目录下执行1npm install hexo-generator-searchdb --save修改站点配置文件添加12345search: path: search.xml field: post format: html limit: 10000修改 next 主题配置文件找到 local_search 修改为 true12local_search: enable: true效果重启 hexo，可以看到在目录栏最下方出现了“搜索”菜单点击弹出框，就可以搜索被Google和百度收录google登陆 google search consolegoogle search console添加你的网站地址（需要google账号）进行验证google 需要验证你拥有该网站的权限，默认推荐的验证方式是在你的网站添加一个它提供的 html，但是由于 hexo 的静态文件是生成的，我们 clean 之后就没了，所以我们不适用这种方式（其实也可以做到）。我们使用另一种更加方便的方式。使用 meta 标签做法是修改主题配置文件，找到 google_site_verification，值修改为 google 提供的 meta 中 content 的内容1google_site_verification: xxxxx加了这个配置后 next 会自动帮我们插入 meta 标签了。我们重启，发布。然后点击上图的验证按钮，成功的话，就会看到以下提示然后我们点击“前往资源页面”，对我们网站其中一个页面进行检查，会提示站点不适用增加站点地图安装插件1npm install hexo-generator-sitemap --save在站点配置文件添加12sitemap: path: sitemap.xml修改站点配置文件，找到 url 项，改为你网站地址。默认是1http://yoursite.com如果你不修改这个，sitemap.xml 生成内容不正确。1url: https://albenw.github.io重新生成、发布，可以看到在 public 目录下生成了 sitemap.xml 文件。在 google search console 提交站点地图提交后结果，看到成功的状态在覆盖率可以看到 google 抓取你的页面，但是我们刚刚添加的网站还没被抓取，要等搜索引擎下一次更新索引你才能在 google 上搜到，请耐性等待。添加 robot.txt原来我是漏掉这一步，了解后发现原来这个文件对爬虫的抓取有一定的帮助，这也是SEO的优化，所以加上。在站点 source 目录下创建 robots.txt，内容如下：1234567891011121314# hexo robots.txtUser-agent: *Allow: /Allow: /archives/Disallow: /vendors/Disallow: /js/Disallow: /css/Disallow: /fonts/Disallow: /vendors/Disallow: /fancybox/Sitemap: https://albenw.github.io/sitemap.xmlSitemap: https://albenw.github.io/baidusitemap.xmlbaidu打开百度的站点管理百度站点管理添加一个站点验证同样我们使用 meta 标签验证修改主题配置文件，添加 baidu_site_verification 项，值为 content 内容。1baidu_site_verification: xxx注意，原来配置文件是没有 baidu_site_verification 这个项的，但是通过查看 layout/_partials/head.swig，我们发现其实 hexo 是支持的，如果 head.swig 没有，则需要我们手动在 head.swig 增加123&#123;% if theme.baidu_site_verification %&#125; &lt;meta name=&quot;baidu-site-verification&quot; content=&quot;&#123;&#123; theme.baidu_site_verification &#125;&#125;&quot; /&gt;&#123;% endif %&#125;配置好后，重新生成，发布，在点击百度站点页面的验证按钮。主动推送由于 github 禁止了百度的爬虫，所以我们不能像对 google 那样通过 sitemap 的方式被抓取到链接，即使你配置了也是没用的。除了 sitemap 还有主动推动和自动推送这两种方式，主动推动的原理是每次 deploy 的时候都把所有链接推送给百度，自动则是每次网站被访问时都把该链接推送给百度。通过对比，我觉得主动推动比价好，所以选用这种方式插件安装1npm install hexo-baidu-url-submit --save修改站点配置文件在 _config.yml，添加以下内容12345baidu_url_submit: count: 5 host: your_site token: your_token path: baidu_urls.txt其中 count 表示一次推送提交最新的N个链接；host 和 token 可以在百度站点页面-&gt;数据引入-&gt;链接提交可以找到；path 为生成的文件名，里面存有推送的，我们网站的链接地址。确保站点配置文件中的 url 项跟百度注册的站点一致同样修改站点配置文件的 deploy 项，我们原来已经有 git 的 deploy，现在增加对 baidu 的推送，最终是这样子的12345deploy:- type: git repo: git@github.com:albenw/albenw.github.io.git branch: master- type: baidu_url_submitter重新生成，发布 hexo d，可以看到推送给百度成功我们可以在百度站点页面-&gt;数据引入-&gt;链接提交看到成交推送的链接数量，不过还不能看当天的，要等明天。（一天后）可以看到提交量了。虽然推送成功了，但是百度不是马上抓取的，需要耐心等待，具体可以查看数据监控-&gt;索引量页面。留言板所谓留言板其实就是开一个空的 page ，然后可以有评论这样子。添加留言板的 page1hexo new page guestbook修改主题配置文件找到 next 的 _config.yml 文件里面的 menu 项，增加1guestbook: /guestbook因为这里使用的是中文，找到 next 主题的 languages 目录里面的zh-Hans.yml文件，menu子项中添加1guestbook: 留言设置留言板的图标next 主题的默认是 page 的名字就是图标 icon 的名字，由于没有 “guestbook” 这个 icon ，所以留言板左边的小图标是一个问号。由于 next 支持 Font Awesome 的所有图标，所以只需要到 Font Awesome 的网站找到你想要的图标，然后还是在主题配置文件的 menu 项，最终修改为1guestbook: /guestbook || comment效果page 默认是开了 comments 的，所以直接用就可以了。参考资料为NexT主题添加文章阅读量统计功能https://blog.csdn.net/blue_zy/article/details/79071414https://www.jianshu.com/p/baea8c95e39bhttp://theme-next.iissnan.com/third-party-services.html#local-searchhttps://www.jianshu.com/p/25145964abf3https://www.jianshu.com/p/5e68f78c7791Hexo插件之百度主动提交链接Hexo博客提交百度和Google收录]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>hexo高级</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo优化]]></title>
    <url>%2Fposts%2F3460d887%2F</url>
    <content type="text"><![CDATA[概要按照我之前hexo的安装部署，可以正常使用，但是或者存在性能或效率的问题，又或者在操作上不便，这篇文章希望能做一点优化和改善。优化图片插入与存放问题一般来说有一下两种方式图床就是图片的云存储，图片存放在云上，这种方式一般是先把图片上传上去，获取到链接，然后在 MD 中引用。我个人觉得这种做法操作麻烦，使用图床麻烦，要先上传图片又麻烦，而且如果图床不稳定，你的图片就可能显示不出来了，甚至图床挂了，你的图片就没了。本地可能我习惯用云笔记，我个人偏向使用在本地的。本地的做法一般是先把图片放在 hexo 站点的目录下，然后在 MD 中引用，这样也可以把图片上传到 github 做备份保存。但我觉得还是有两个问题，一是操作麻烦，二是管理图片。第一，我个人喜欢用 hexo-admin，直接在页面复制图片就行了。第二，hexo-admin 默认是放在 images 目录下的，但是如果文章越来越多，图片会很乱。关于这点 hexo 提供 post_asset_folder 参数配置，为 true 的话，在新建 post 时会在 _post 建一个同名文件夹（仅此而已），hexo 的初衷是想我们把图片放在里面，可惜 hexo-admin 对这个配置还不支持，我看它还是在 issue 里。所以到这里，我觉得还是没有好的做法，我自己的做法是放 images 目录，图片的命名要有规范，例如 post_name + “__“ + index 这样，方便做管理。这里提醒大家一点，编辑时使用的图片的路径和生成 html 时的是不一样的。html，js，css，images 压缩使用 hexo-all-minifier 插件，在站点目录执行1npm install hexo-all-minifier --save在站点配置文件 _config.yml ，增加一行即可1all_minifier: true在 hexo g 生成的时候会看到打印输出 xx% saved 这样的字眼，表示成功了。我觉得感觉是好像是。。快了一点。。吧。。文章唯一link更改文章题目或者变更文章发布时间，在默认设置下，文章链接都会改变，不利于搜索引擎收录，也不利于分享。这里还是涉及爬虫的知识点，如果链接的层级太深，则对SEO不友好。所以简短的、唯一永久链接才是更好的选择。安装插件1npm install hexo-abbrlink --save在站点配置文件中查找代码permalink，将其更改为1permalink: posts/:abbrlink/ # “posts/” 可自行更换修改配置然后在站点配置文件中添加如下代码1234# abbrlink configabbrlink: alg: crc32 # 算法：crc16(default) and crc32 rep: hex # 进制：dec(default) and hex效果重启 hexo 生效后，可以看到文章的链接不再是“日期+文章名”，而是配置的 permalink，后面的一串字符就是 abbrlink。特别的说明：由于加了这个配置之后文章的链接URL变了，所以之前如果有做“评论”或“访问计数”配置的，就会全部失效。预览首页进去是对每一篇文章都显示了所有内容，需要把当前文章滚动到末尾才能看到下一篇文章，这样不能让读者快速浏览到大概有哪些文章，不能一下子吸引到读者。在主题配置文件中找到 auto_excerpt 属性，将enable设置为true ，将length设置为想要预览到的字数123auto_excerpt:enable: true #将原有的false改为truelength: 300 #设置预览的字数在首页看到的效果图，它的摘要只是把文本存粹的按照 length 截取出来。SEO优化做seo优化有利于搜索引擎对你网站的索引，根据关键字提高你网站的排名，提高曝光率。title 优化使首页改为“网站名称-网站描述”这样的显示方式。打开 seo 项在主题配置文件找到 seo 项1seo: true修改 post 模版在站点目录 scaffolds\post.md 文件，添加 keywords 和 description 字段，用于生成的文章中添加关键字和描述。123456title: &#123;&#123; title &#125;&#125;date: &#123;&#123; date &#125;&#125;tags:keywords:description:---这样在首页文章的预览中就会变成 description，利于 SEO。添加 “nofollow” 标签nofollow是HTML的一个属性，用于告诉搜索引擎不要追踪特定的网页链接。可以用于阻止在PR值高的网站上以留言等方式添加链接从而提高自身网站排名的行为，以改善搜索结果的质量，防止垃圾链接的蔓延。网站站长也可对其网页中的付费链接使用nofollow来防止该链接降低搜索排名。对一些重要度低的网页内容使用nofollow，还可以使搜索引擎以不同的优先级别来抓取网页内容。by 维基百科修改footer.swig文件在 next 目录 layout_partials，找到两处 a标签加上 rel=”external nofollow” 属性。1&#123;&#123; __(&apos;footer.powered&apos;, &apos;&lt;a rel=&quot;external nofollow&quot; class=&quot;theme-link&quot; target=&quot;_blank&quot; href=&quot;https://hexo.io&quot;&gt;Hexo&lt;/a&gt;&apos;) &#125;&#125;1&lt;a rel=&quot;external nofollow&quot; class=&quot;theme-link&quot; target=&quot;_blank&quot; href=&quot;https://github.com/iissnan/hexo-theme-next&quot;&gt;修改sidebar.swig文件在 next 目录 layout_macro，将下面代码中的a标签加上rel=”external nofollow”属性，顺序如下。1&lt;a rel=&quot;external nofollow&quot; href=&quot;&#123;&#123; link.split(&apos;||&apos;)[0] | trim &#125;&#125;&quot; target=&quot;_blank&quot; title=&quot;&#123;&#123; name &#125;&#125;&quot;&gt;1&lt;a href=&quot;https://creativecommons.org/&#123;% if theme.creative_commons === &apos;zero&apos; %&#125;publicdomain/zero/1.0&#123;% else %&#125;licenses/&#123;&#123; theme.creative_commons &#125;&#125;/4.0&#123;% endif %&#125;/&quot; rel=&quot;external nofollow&quot; class=&quot;cc-opacity&quot; target=&quot;_blank&quot;&gt;1&lt;a href=&quot;&#123;&#123; link &#125;&#125;&quot; title=&quot;&#123;&#123; name &#125;&#125;&quot; rel=&quot;external nofollow&quot; target=&quot;_blank&quot;&gt;&#123;&#123; name &#125;&#125;&lt;/a&gt;其实就是把一些含有 target=”_blank” 或 链去其他网站的超链接给加上 nofollow ，提升 SEO 效率。唯一链接 permalink这个我们在上面已经做了。小技巧文章内引用自己的文章这是hexo的标签语法1234&#123;% post_link 文章文件名（不要后缀） 文章标题（可选） %&#125;&#123;% post_link Hello-World %&#125;&#123;% post_link Hello-World 你好世界 %&#125;注意文章名字和文章标题不能有空格，有的话不能生效，还不知怎么解决。直到我增加了站内搜索功能后，好奇搜索结果是怎么链接到文章的呢，于是我看了一下，如下1&lt;a href=&quot;/2018/09/04/Hexo-Github-Pages安装部署/&quot; class=&quot;search-result-title&quot;&gt;&lt;b class=&quot;search-keyword&quot;&gt;Hexo&lt;/b&gt;+Github Pages安装部署&lt;/a&gt;可以看出原来 post 的名字是 “Hexo+Github Pages安装部署”，但是生成静态页面就变成了 “Hexo-Github-Pages安装部署”。然后我拿这个 “Hexo-Github-Pages安装部署”放到上面一试，发现可以了！看来生成后 post 的名字如果单词之间有特殊符号会统一变成“-”？？插入图片在 hexo-admin 直接复制图片会是这样子1![upload successful](/images/hexo优化__0.png)但是这样直接显示在页面不适合，我们一般需要调整大小或位置调整图片的显示hexo 支持的标签语法1&#123;% img [class names] /path/to/image [width] [height] [title text [alt text]] %&#125;不过不能在 hexo-amdin 看到。img 标签1&lt;img src=&quot;/images/hexo-admin安装使用__0.png&quot; width=&quot;600px&quot; height=&quot;200px&quot; align=center&gt;这样可以在直接 hexo-admin 中显示，路径也兼容生成后的 html。]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexo-admin安装使用]]></title>
    <url>%2Fposts%2F4ffa5bc6%2F</url>
    <content type="text"><![CDATA[概要如果自己编辑 MD 文件的话，确实比较麻烦，你可以用一些 MD 的编辑器，但是在管理 MD 文件上还是操作不方便。这里推荐使用 hexo-admin，而且编辑完之后可以马上看到效果呢。需要说明的是，hexo-admin 管理是本地用的，就是你需要在本地编辑完之后再上传到 github，而不能直接在线编辑保存，因为 github pages 只支持静态页面的。安装过程安装过程中可能涉及到一些前提或内容，请参考我的另一篇文章Hexo-Github-Pages安装部署前提基于版本”hexo”: “^3.7.0”，”hexo-admin”: “^2.3.0”。安装 hexo-admincd hexo 目录1npm install --save hexo-admin启动 hexo1hexo s然后打开 http://localhost:4000/admin/ 就可以看到管理页面。在 hexo-admin 你可以Pages - 新加 page；Posts - 新加或删除 post；双击一个 post，你可以编辑，预览，新增修改 tags、categories，选择发布或不发布；Settings - 一些配置；Deploy - 可以直接部署到 github。问题minimatch1npm WARN deprecated minimatch@2.0.10: Please update to minimatch 3.0.2 or higher to avoid a RegExp DoS issue当你安装 hexo-admin，执行 npm install –save hexo-admin 时，可能会遇到上面的错误提示，是因为你缺少了一些依赖，执行下面的就好了。12npm install minimatch@&quot;3.0.2&quot; npm update -dConfig value “admin.deployCommand” not found当你第一次点击 Deploy 按钮时，可能会遇到上述的错误，因为缺少了执行 deploy 的命令，这个问题已经有人提了 issue 并且解决了https://github.com/jaredly/hexo-admin/issues/70还需要注意的是，issue 中的脚本只是 hexo deploy，只是做 deploy 操作，但是一般我们的使用习惯是编辑完之后 deploy，所以是要 deploy 最新的，需要把脚本改为即可123#!/usr/bin/env shhexo ghexo ddeploy 后你可能看到1234Std Error(node:83411) [DEP0061] DeprecationWarning: fs.SyncWriteStream is deprecated.Warning: Permanently added the RSA host key for IP address &apos;13.229.188.59&apos; to the list of known hosts.Everything up-to-date这不是错误，你可以不用管。说明已经 deploy 成功。复制图片时的一个小问题hexo-admin 编辑时支持直接复制图片（截图）到内容，这点是我比较喜欢的。但是有个问题，复制进去后是加载不出来的，会出现图裂的小图标。这时你只需要点击别的页面，再点回来就可以看到了，就是“刷”一下就好了，最简单的就是点击右上角打勾的按钮，这个按钮的作用是拼写检查，点一下再点回来，就可以看到你刚复制进去的图片了。这大概是因为 hexo-admin 对图片做了延迟加载，具体可以看看这篇文章说的https://htchz.me/2018/03/10/Hexo/参考资料https://www.jianshu.com/p/68e727dda16dhttps://blog.kinpzz.com/2016/12/31/hexo-admin-backend-management/https://github.com/jaredly/hexo-admin]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo-admin</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+Github Pages安装部署]]></title>
    <url>%2Fposts%2F3454819c%2F</url>
    <content type="text"><![CDATA[概要想低成本的搞一个博客，在网上找了很多参考资料，于是尝试 Hexo+Github Pages 折腾一下。我把自己的搭建过程记录下来，把其中遇到的一些问题也跟大家分享。安装过程前提我用的是 macOS 系统；node、npm、git 等的安装，还有 github 的配置就不多讲了；基于 hexo 的 3.7.0 版本。安装 hexo 客户端1npm install -g hexo-cli创建一个用来放 hexo 的文件夹（假设为 hexo）cd 进去，创建 hexo 站点1hexo init使用 next 主题为了博客的美观和使用上方面，我使用的大众的 next 主题。cd themes 目录。下载 next 主题1git clone https://github.com/iissnan/hexo-theme-next修改 theme编辑 hexo/_config.yml，找到 theme 那一行配置，修改为 next本地启动看看安装完之后，我们可以在本地启动看看博客初始化的效果。生成静态文件1hexo ghexo 最终运行的是静态文件，包括js，css和html等，这些文件统一放在 public 文件夹。安装 hexo-server1npm install hexo-server --save启动 server1hexo s在浏览器打开 localhost:4000，会看到一个 Hello World的页面。恭喜你，部署成功。部署到 github把 hexo 生成的静态文件上传到 github，别人就可以在 github 的网站上看到你的博客了。创建 repo在 github 上创建一个仓库，repo的名字为 username.github.io安装 deploy 插件cd 到 hexo 目录，执行1npm install hexo-deployer-git --save修改 deploy 相关配置编辑 hexo/_config.yml，修改 deploy 下几个属性123type: gitrepo: （git地址）branch: masterpush 到 github1hexo d就会自动把 public 文件夹下所有内容 push 到 master。注意这里看一下 git config user.name\email 是否正确。打开网页打开 username.github.io 就可以看到了添加“分类”，“关于”和“标签”菜单到此已经把博客基本的安装和部署好了。但是我们还需要做一些基本配置，让我们可以维护博客。打开 tags，about，categories在主题配置文件 next/_config.yml 在 menu 下去掉 tags，about，categories 注释。注意这里“主题配置文件”指的是 themes/next 目录的下的 _config.yml。创建 tags，about，categories在 hexo 文件夹1hexo new page tags会在 source 文件夹生成 tags 文件夹，编辑里面的 index.md ，添加12type: "tags"comments: false同样的方法添加 categories；添加 about 不需要修改 md 文件的 type，因为 tags，categories 是特殊目录类型，about 只是简单的一个 md。为文章添加标签和分类在文章 md 文件开头 title 的下面，增加类似，就可以归类到 tag 和 category1234tag:- a_tagcategories:- a_category添加头像图片在 hexo/_config.yml 找到配置 avatar，增加图片路径1avatar: /images/avatar.jpeg新建文章1hexo new post new1就会在 source/_post 文件夹下生成 new1.md 文件，编辑 md 文件即可。这里为什么是 post ？这里涉及 hexo 的模版行为，在 scaffolds 目录下初始定义了3个模板，draft、page、post，文章就是用到了 post。代码管理首先要搞清楚，hexo d 会把 public 文件夹 push 到 username.github.io 这个 repo 的 master 分支。但是这些文件都是一些生成出来的html，css，js 等，对我们没用，所以我们需要把原始文件如 md，images，_config.yml 等文件也需要保存下来，说白了就是把上述的文件也上传到 github，但是我们已经把 public push 到 master了，这时我们可以在 github 上再建一个 repo 来放我们的代码，我的选择是在 username.github.io 上建一个分支放，其实操作是差不多的。其实，我们可以发现在 hexo 文件夹下有一个 .gitignore 文件，这时 hexo 帮我们准备好的，里面的内容：1234567.DS_StoreThumbs.dbdb.json*.lognode_modules/public/.deploy*/可以看出 hexo 已经为我们想好了，public、node_modules、.deploy_git 等非源码文件都忽略了。具体自己看情况，这个 .gitignore 我没动。有个坑下载下来的是一个 git 库，如果你等下把整个 next 文件夹 push 的话，那么在 github 上 next 文件夹是灰色的，你是操作不了，这可能跟 github 权限有关。所以你要先把 next 下的 .git 文件夹删掉。在 hexo 文件夹执行12345678git init git add .git commit -m "hexo-src init"git branch hexo-srcgit checkout hexo-srcgit remote add origin （username.github.io 的 repo git 地址）git push -f origin hexo-src - 强推上去git branch --set-upstream hexo-src origin/hexo-src - 关联上好了，以后改完文章或者修改完主题配置，就可以 push 到 github 了。参考资料https://blog.csdn.net/u012195214/article/details/79204088http://www.wuxubj.cn/2016/08/Hexo-nexT-build-personal-blog/#https://zhiho.github.io/2015/09/29/hexo-next/http://theme-next.iissnan.com/getting-started.htmlhttp://www.lzblog.cn/2016/04/07/Hexo%E7%AB%99%E7%82%B9%E3%80%81NexT%E4%B8%BB%E9%A2%98%E4%BF%AE%E6%94%B9%E5%85%A8%E8%AE%B0%E5%BD%95/https://codezjx.com/2017/07/31/hexo-guide/]]></content>
      <categories>
        <category>hexo</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fposts%2F4a17b156%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.Quick StartCreate a new post1$ hexo new "My New Post"More info: WritingRun server1$ hexo serverMore info: ServerGenerate static files1$ hexo generateMore info: GeneratingDeploy to remote sites1$ hexo deployMore info: Deployment]]></content>
      <categories>
        <category>index</category>
      </categories>
      <tags>
        <tag>index</tag>
      </tags>
  </entry>
</search>
